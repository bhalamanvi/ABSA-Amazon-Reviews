{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7d6968a-fdf2-4c2d-942d-8355d9c9caa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading All Beauty reviews...\n",
      "Loaded 701528 reviews from All Beauty\n",
      "Loading Health and Personal Care reviews...\n",
      "Loaded 494121 reviews from Health and Personal Care\n",
      "Loading Appliances reviews...\n",
      "Loaded 2128605 reviews from Appliances\n",
      "Loading Video_Games reviews...\n",
      "Loaded 4624615 reviews from Video_Games\n",
      "Loading All Beauty metadata...\n",
      "Loaded 112590 product metadata from All Beauty\n",
      "Loading Health and Personal Care metadata...\n",
      "Loaded 60293 product metadata from Health and Personal Care\n",
      "Loading Appliances metadata...\n",
      "Loaded 94327 product metadata from Appliances\n",
      "Loading Video_Games metadata...\n",
      "Loaded 137269 product metadata from Video_Games\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Bhala\\AppData\\Local\\Temp\\ipykernel_3816\\2501278792.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  reviews_df['reviewText'] = reviews_df['text']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reviews loaded: 10000\n",
      "Total products loaded: 404479\n",
      "Merged dataset size: 10000\n",
      "Sample data:\n",
      "   rating                                       review_title  \\\n",
      "0     5.0          A fabulous new installment to the series!   \n",
      "1     5.0  Waterloo: Tabletop Wargaming in the Age of Nap...   \n",
      "2     5.0                                Very easy to instal   \n",
      "3     5.0                                   Easy to install.   \n",
      "4     1.0                                             sucks!   \n",
      "\n",
      "                                                text images        asin  \\\n",
      "0  I LOVE this game. Really looking forward to ne...     []  B01N3NNPAB   \n",
      "1  My son has been a Warhammer enthusiast for the...     []  1907964177   \n",
      "2                                       Refrigerator     []  B06VW9HKXF   \n",
      "3                                   Easy to install.     []  B07P3B366X   \n",
      "4  Cheap plastic. Soft case. My son broke in with...     []  B007PX6MFM   \n",
      "\n",
      "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
      "0  B01GY35HKE  AF4H3UFUUEDLMESMC3SCBPQWUIQA  1494468180000             0   \n",
      "1  1907964177  AE4PES27ANXCRHV3NSYDL3Z62ZZA  1325389401000             1   \n",
      "2  B06VW9HKXF  AH4ULRXVQOWLDB2DWIVIGK3WFEHQ  1674241249612             0   \n",
      "3  B07P3B366X  AGJJVKCCOTLJC7AGIFDNP4AECTRA  1680651776518             0   \n",
      "4  B007PX6MFM  AHB66TQ4PPSZG2EDUUBEGTRZCOCQ  1419384559000             0   \n",
      "\n",
      "   verified_purchase     category                datetime  \\\n",
      "0              False  Video Games 2017-05-11 07:33:00.000   \n",
      "1              False  Video Games 2012-01-01 09:13:21.000   \n",
      "2               True   Appliances 2023-01-21 00:30:49.612   \n",
      "3               True   Appliances 2023-04-05 05:12:56.518   \n",
      "4               True  Video Games 2014-12-24 06:59:19.000   \n",
      "\n",
      "                                          reviewText  \\\n",
      "0  I LOVE this game. Really looking forward to ne...   \n",
      "1  My son has been a Warhammer enthusiast for the...   \n",
      "2                                       Refrigerator   \n",
      "3                                   Easy to install.   \n",
      "4  Cheap plastic. Soft case. My son broke in with...   \n",
      "\n",
      "              main_category  \\\n",
      "0               Video Games   \n",
      "1                     Books   \n",
      "2                Appliances   \n",
      "3  Tools & Home Improvement   \n",
      "4                 Computers   \n",
      "\n",
      "                                       product_title  average_rating  \\\n",
      "0            Mass Effect Andromeda Deluxe - Xbox One             4.3   \n",
      "1                     Warhammer Historical: Waterloo             5.0   \n",
      "2  GOLDEN ICEPURE DA29-00020B Refrigerator Water ...             4.7   \n",
      "3  LIFETIME WARRANTY WR57X10051 Refrigerator Dual...             4.6   \n",
      "4  Insten 28-in-1 Game Card Case Compatible with ...             4.5   \n",
      "\n",
      "                     store  \n",
      "0          Electronic Arts  \n",
      "1  Mark A. Latham (Author)  \n",
      "2           GOLDEN ICEPURE  \n",
      "3                BlueStars  \n",
      "4                 eForCity  \n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def load_amazon_reviews_hf(categories):\n",
    "    \"\"\"\n",
    "    Load Amazon reviews for specific categories using Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        categories: List of categories to include (e.g., \"All_Beauty\", \"Appliances\")\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with filtered reviews\n",
    "    \"\"\"\n",
    "    all_reviews = []\n",
    "    \n",
    "    for category in categories:\n",
    "        try:\n",
    "            # Format category name for dataset loading\n",
    "            formatted_category = category.replace(\" \", \"_\")\n",
    "            dataset_name = f\"raw_review_{formatted_category}\"\n",
    "            \n",
    "            print(f\"Loading {category} reviews...\")\n",
    "            dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", dataset_name, split=\"full\", trust_remote_code=True)\n",
    "            \n",
    "            # Convert to pandas DataFrame\n",
    "            category_reviews = pd.DataFrame(dataset)\n",
    "            \n",
    "            # Add readable category name\n",
    "            category_reviews['category'] = category.replace(\"_\", \" \")\n",
    "            \n",
    "            # Convert timestamp to datetime\n",
    "            category_reviews['datetime'] = category_reviews['timestamp'].apply(\n",
    "                lambda x: datetime.fromtimestamp(x/1000)\n",
    "            )\n",
    "            \n",
    "            all_reviews.append(category_reviews)\n",
    "            print(f\"Loaded {len(category_reviews)} reviews from {category}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {category}: {e}\")\n",
    "    \n",
    "    if all_reviews:\n",
    "        combined_reviews = pd.concat(all_reviews, ignore_index=True)\n",
    "        return combined_reviews\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_amazon_metadata_hf(categories):\n",
    "    \"\"\"\n",
    "    Load Amazon product metadata for specific categories using Hugging Face datasets.\n",
    "    \n",
    "    Args:\n",
    "        categories: List of categories to include (e.g., \"All_Beauty\", \"Appliances\")\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with product metadata\n",
    "    \"\"\"\n",
    "    all_metadata = []\n",
    "    \n",
    "    for category in categories:\n",
    "        try:\n",
    "            # Format category name for dataset loading\n",
    "            formatted_category = category.replace(\" \", \"_\")\n",
    "            dataset_name = f\"raw_meta_{formatted_category}\"\n",
    "            \n",
    "            print(f\"Loading {category} metadata...\")\n",
    "            dataset = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", dataset_name, split=\"full\", trust_remote_code=True)\n",
    "            \n",
    "            # Convert to pandas DataFrame\n",
    "            category_metadata = pd.DataFrame(dataset)\n",
    "            \n",
    "            # Add readable category name\n",
    "            category_metadata['category'] = category.replace(\"_\", \" \")\n",
    "            \n",
    "            all_metadata.append(category_metadata)\n",
    "            print(f\"Loaded {len(category_metadata)} product metadata from {category}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {category} metadata: {e}\")\n",
    "    \n",
    "    if all_metadata:\n",
    "        combined_metadata = pd.concat(all_metadata, ignore_index=True)\n",
    "        return combined_metadata\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def preprocess_reviews(reviews_df, sample_size=None):\n",
    "    \"\"\"\n",
    "    Preprocess the reviews dataframe.\n",
    "    \n",
    "    Args:\n",
    "        reviews_df: DataFrame with reviews\n",
    "        sample_size: Number of reviews to sample (optional)\n",
    "    \n",
    "    Returns:\n",
    "        Preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Remove reviews with empty text\n",
    "    reviews_df = reviews_df[reviews_df['text'].notna() & (reviews_df['text'] != \"\")]\n",
    "    \n",
    "    # Create a 'reviewText' column for compatibility with the rest of the code\n",
    "    reviews_df['reviewText'] = reviews_df['text']\n",
    "    \n",
    "    # Sample if needed\n",
    "    if sample_size and len(reviews_df) > sample_size:\n",
    "        reviews_df = reviews_df.sample(sample_size, random_state=42)\n",
    "    \n",
    "    return reviews_df\n",
    "\n",
    "def merge_reviews_with_metadata(reviews_df, metadata_df):\n",
    "    \"\"\"\n",
    "    Merge reviews with product metadata.\n",
    "    \n",
    "    Args:\n",
    "        reviews_df: DataFrame with reviews\n",
    "        metadata_df: DataFrame with product metadata\n",
    "    \n",
    "    Returns:\n",
    "        Merged DataFrame\n",
    "    \"\"\"\n",
    "    # Merge on parent_asin\n",
    "    merged_df = pd.merge(\n",
    "        reviews_df,\n",
    "        metadata_df[['parent_asin', 'main_category', 'title', 'average_rating', 'store']],\n",
    "        on='parent_asin',\n",
    "        how='left',\n",
    "        suffixes=('_review', '_product')\n",
    "    )\n",
    "    \n",
    "    # Rename columns for clarity\n",
    "    merged_df = merged_df.rename(columns={\n",
    "        'title_review': 'review_title',\n",
    "        'title_product': 'product_title'\n",
    "    })\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    categories = [\n",
    "        \"All Beauty\", \n",
    "        \"Health and Personal Care\", \n",
    "        \"Appliances\",\n",
    "        \"Video_Games\"\n",
    "    ]\n",
    "    \n",
    "    # Load reviews\n",
    "    reviews_df = load_amazon_reviews_hf(categories)\n",
    "    \n",
    "    # Load metadata\n",
    "    metadata_df = load_amazon_metadata_hf(categories)\n",
    "    \n",
    "    # Preprocess reviews\n",
    "    reviews_df = preprocess_reviews(reviews_df, sample_size=10000)\n",
    "    \n",
    "    # Merge reviews with metadata\n",
    "    merged_df = merge_reviews_with_metadata(reviews_df, metadata_df)\n",
    "    \n",
    "    print(f\"Total reviews loaded: {len(reviews_df)}\")\n",
    "    print(f\"Total products loaded: {len(metadata_df)}\")\n",
    "    print(f\"Merged dataset size: {len(merged_df)}\")\n",
    "    print(f\"Sample data:\\n{merged_df.head()}\")\n",
    "    \n",
    "    # Save the filtered dataset\n",
    "    merged_df.to_parquet(\"filtered_amazon_reviews.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a01760ad-ed61-4476-aedb-f54270fa116d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Manvi\n",
      "[nltk_data]     Bhala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Review: This shampoo has a great fragrance and leaves my hair feeling soft.\n",
      "Category: All Beauty\n",
      "Detected aspects: ['fragrance', 'hair']\n",
      "\n",
      "Review: The vacuum cleaner is powerful but very noisy.\n",
      "Category: Appliances\n",
      "Detected aspects: ['energy_consumption', 'sound', 'maintenance', 'controls', 'setup', 'power', 'warranty', 'weight', 'manual', 'noise', 'settings', 'design', 'cleaning', 'features', 'ease_of_use', 'customer_service', 'instructions', 'build_quality', 'size', 'durability', 'reliability', 'performance', 'efficiency', 'dimensions', 'installation']\n",
      "\n",
      "Review: This game has amazing graphics but the story is weak.\n",
      "Category: Video_Games\n",
      "Detected aspects: ['glitches', 'replayability', 'voice_acting', 'story', 'graphics', 'dlc', 'gameplay', 'multiplayer']\n",
      "\n",
      "Review: The vitamins are effective but expensive.\n",
      "Category: Health and Personal Care\n",
      "Detected aspects: ['effectiveness', 'side_effects', 'ease_of_use', 'customer_service', 'ingredients', 'taste', 'organic', 'scent', 'smell', 'dosage', 'natural', 'flavor', 'chemicals']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class AspectDetector:\n",
    "    \"\"\"\n",
    "    Class for detecting aspects in reviews using rule-based and model-based approaches.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the AspectDetector with category-specific aspects.\n",
    "        \"\"\"\n",
    "        # Define aspects for each category\n",
    "        self.category_aspects = {\n",
    "            \"All Beauty\": [\n",
    "                \"fragrance\", \"scent\", \"smell\", \"texture\", \"consistency\", \"absorption\", \n",
    "                \"packaging\", \"bottle\", \"container\", \"applicator\", \"effectiveness\", \n",
    "                \"results\", \"skin\", \"face\", \"hair\", \"nails\", \"ingredients\", \"natural\", \n",
    "                \"organic\", \"chemicals\", \"allergic\", \"reaction\", \"price\", \"value\", \n",
    "                \"quantity\", \"size\", \"brand\", \"customer_service\"\n",
    "            ],\n",
    "            \"Health and Personal Care\": [\n",
    "                \"effectiveness\", \"results\", \"side_effects\", \"ingredients\", \"natural\", \n",
    "                \"organic\", \"chemicals\", \"taste\", \"flavor\", \"smell\", \"scent\", \"texture\", \n",
    "                \"consistency\", \"ease_of_use\", \"convenience\", \"packaging\", \"dosage\", \n",
    "                \"instructions\", \"price\", \"value\", \"quantity\", \"size\", \"brand\", \n",
    "                \"customer_service\", \"shipping\", \"delivery\"\n",
    "            ],\n",
    "            \"Appliances\": [\n",
    "                \"performance\", \"power\", \"efficiency\", \"noise\", \"sound\", \"volume\", \n",
    "                \"size\", \"dimensions\", \"weight\", \"design\", \"appearance\", \"color\", \n",
    "                \"material\", \"build_quality\", \"durability\", \"reliability\", \"features\", \n",
    "                \"functions\", \"settings\", \"controls\", \"ease_of_use\", \"installation\", \n",
    "                \"setup\", \"instructions\", \"manual\", \"price\", \"value\", \"warranty\", \n",
    "                \"customer_service\", \"energy_consumption\", \"maintenance\", \"cleaning\"\n",
    "            ],\n",
    "            \"Video_Games\": [\n",
    "                \"gameplay\", \"mechanics\", \"controls\", \"difficulty\", \"challenge\", \n",
    "                \"story\", \"plot\", \"narrative\", \"characters\", \"graphics\", \"visuals\", \n",
    "                \"animation\", \"sound\", \"music\", \"voice_acting\", \"performance\", \n",
    "                \"loading_times\", \"bugs\", \"glitches\", \"multiplayer\", \"online\", \n",
    "                \"community\", \"replayability\", \"content\", \"dlc\", \"price\", \"value\", \n",
    "                \"physical_copy\", \"digital_download\", \"installation\"\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        # Initialize tokenizer and model for embedding-based detection\n",
    "        try:\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "            self.model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "        except:\n",
    "            print(\"Warning: Could not load transformer model. Using rule-based detection only.\")\n",
    "            self.tokenizer = None\n",
    "            self.model = None\n",
    "        \n",
    "        # Initialize stopwords\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def detect_aspects(self, review, category=None):\n",
    "        \"\"\"\n",
    "        Detect aspects in a review using both rule-based and embedding-based approaches.\n",
    "        \n",
    "        Args:\n",
    "            review (str): The review text.\n",
    "            category (str, optional): The product category.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected aspects.\n",
    "        \"\"\"\n",
    "        # Use rule-based detection\n",
    "        rule_based_aspects = self.extract_aspects_rule_based(review, category)\n",
    "        \n",
    "        # Use embedding-based detection if model is available\n",
    "        if self.tokenizer is not None and self.model is not None:\n",
    "            embedding_based_aspects = self.extract_aspects_embedding_based(review, category)\n",
    "            # Combine results\n",
    "            all_aspects = list(set(rule_based_aspects + embedding_based_aspects))\n",
    "        else:\n",
    "            all_aspects = rule_based_aspects\n",
    "        \n",
    "        return all_aspects\n",
    "    \n",
    "    def extract_aspects_rule_based(self, review, category=None):\n",
    "        \"\"\"\n",
    "        Extract aspects from a review using rule-based approach.\n",
    "        \n",
    "        Args:\n",
    "            review (str): The review text.\n",
    "            category (str, optional): The product category.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected aspects.\n",
    "        \"\"\"\n",
    "        if not isinstance(review, str):\n",
    "            return []\n",
    "        \n",
    "        # Lowercase the review\n",
    "        review = review.lower()\n",
    "        \n",
    "        # Tokenize the review\n",
    "        tokens = word_tokenize(review)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        tokens = [token for token in tokens if token not in self.stop_words]\n",
    "        \n",
    "        # Get aspects for the category\n",
    "        if category and category in self.category_aspects:\n",
    "            category_aspects = self.category_aspects[category]\n",
    "        else:\n",
    "            # If category is not provided or not found, use all aspects\n",
    "            category_aspects = []\n",
    "            for aspects in self.category_aspects.values():\n",
    "                category_aspects.extend(aspects)\n",
    "            category_aspects = list(set(category_aspects))\n",
    "        \n",
    "        # Check for aspects in the review\n",
    "        detected_aspects = []\n",
    "        for aspect in category_aspects:\n",
    "            # Convert aspect with underscores to space-separated words\n",
    "            aspect_words = aspect.replace('_', ' ').split()\n",
    "            \n",
    "            # Check if all words in the aspect are in the review\n",
    "            if all(word in review for word in aspect_words):\n",
    "                detected_aspects.append(aspect)\n",
    "        \n",
    "        return detected_aspects\n",
    "    \n",
    "    def extract_aspects_embedding_based(self, review, category=None):\n",
    "        \"\"\"\n",
    "        Extract aspects from a review using embedding-based approach.\n",
    "        \n",
    "        Args:\n",
    "            review (str): The review text.\n",
    "            category (str, optional): The product category.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of detected aspects.\n",
    "        \"\"\"\n",
    "        if not isinstance(review, str) or self.tokenizer is None or self.model is None:\n",
    "            return []\n",
    "        \n",
    "        # Get aspects for the category\n",
    "        if category and category in self.category_aspects:\n",
    "            category_aspects = self.category_aspects[category]\n",
    "        else:\n",
    "            # If category is not provided or not found, use all aspects\n",
    "            category_aspects = []\n",
    "            for aspects in self.category_aspects.values():\n",
    "                category_aspects.extend(aspects)\n",
    "            category_aspects = list(set(category_aspects))\n",
    "        \n",
    "        # Encode the review\n",
    "        inputs = self.tokenizer(review, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        # Get the review embedding (average of token embeddings)\n",
    "        review_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        \n",
    "        # Encode each aspect\n",
    "        aspect_embeddings = {}\n",
    "        for aspect in category_aspects:\n",
    "            # Convert aspect with underscores to space-separated words\n",
    "            aspect_text = aspect.replace('_', ' ')\n",
    "            \n",
    "            inputs = self.tokenizer(aspect_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**inputs)\n",
    "            \n",
    "            # Get the aspect embedding\n",
    "            aspect_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            aspect_embeddings[aspect] = aspect_embedding\n",
    "        \n",
    "        # Calculate cosine similarity between review and each aspect\n",
    "        detected_aspects = []\n",
    "        for aspect, aspect_embedding in aspect_embeddings.items():\n",
    "            # Calculate cosine similarity\n",
    "            similarity = np.dot(review_embedding, aspect_embedding) / (\n",
    "                np.linalg.norm(review_embedding) * np.linalg.norm(aspect_embedding))\n",
    "            \n",
    "            # If similarity is above threshold, consider the aspect as detected\n",
    "            if similarity > 0.5:  # Threshold can be adjusted\n",
    "                detected_aspects.append(aspect)\n",
    "        \n",
    "        return detected_aspects\n",
    "\n",
    "\n",
    "class GNNContextModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network model for context-aware sentiment analysis.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        \"\"\"\n",
    "        Initialize the GNN model.\n",
    "        \n",
    "        Args:\n",
    "            input_dim (int): Input dimension.\n",
    "            hidden_dim (int): Hidden dimension.\n",
    "            output_dim (int): Output dimension.\n",
    "            num_layers (int): Number of GNN layers.\n",
    "        \"\"\"\n",
    "        super(GNNContextModel, self).__init__()\n",
    "        \n",
    "        # Input projection\n",
    "        self.input_proj = nn.Linear(input_dim, hidden_dim)\n",
    "        \n",
    "        # GNN layers\n",
    "        self.gnn_layers = nn.ModuleList()\n",
    "        for _ in range(num_layers):\n",
    "            self.gnn_layers.append(GNNLayer(hidden_dim))\n",
    "        \n",
    "        # Output projection\n",
    "        self.output_proj = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x, adj_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Node features.\n",
    "            adj_matrix (torch.Tensor): Adjacency matrix.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Output features.\n",
    "        \"\"\"\n",
    "        # Input projection\n",
    "        h = F.relu(self.input_proj(x))\n",
    "        \n",
    "        # GNN layers\n",
    "        for layer in self.gnn_layers:\n",
    "            h = layer(h, adj_matrix)\n",
    "        \n",
    "        # Output projection\n",
    "        out = self.output_proj(h)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class GNNLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Graph Neural Network layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim):\n",
    "        \"\"\"\n",
    "        Initialize the GNN layer.\n",
    "        \n",
    "        Args:\n",
    "            hidden_dim (int): Hidden dimension.\n",
    "        \"\"\"\n",
    "        super(GNNLayer, self).__init__()\n",
    "        \n",
    "        # Message passing\n",
    "        self.message = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Update\n",
    "        self.update = nn.Linear(2 * hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x, adj_matrix):\n",
    "        \"\"\"\n",
    "        Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): Node features.\n",
    "            adj_matrix (torch.Tensor): Adjacency matrix.\n",
    "            \n",
    "        Returns:\n",
    "            torch.Tensor: Updated node features.\n",
    "        \"\"\"\n",
    "        # Message passing\n",
    "        m = self.message(x)\n",
    "        m = torch.matmul(adj_matrix, m)\n",
    "        \n",
    "        # Update\n",
    "        h = torch.cat([x, m], dim=1)\n",
    "        h = self.update(h)\n",
    "        h = F.relu(h)\n",
    "        \n",
    "        return h\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize aspect detector\n",
    "    aspect_detector = AspectDetector()\n",
    "    \n",
    "    # Example reviews\n",
    "    reviews = [\n",
    "        \"This shampoo has a great fragrance and leaves my hair feeling soft.\",\n",
    "        \"The vacuum cleaner is powerful but very noisy.\",\n",
    "        \"This game has amazing graphics but the story is weak.\",\n",
    "        \"The vitamins are effective but expensive.\"\n",
    "    ]\n",
    "    \n",
    "    categories = [\n",
    "        \"All Beauty\",\n",
    "        \"Appliances\",\n",
    "        \"Video_Games\",\n",
    "        \"Health and Personal Care\"\n",
    "    ]\n",
    "    \n",
    "    # Detect aspects\n",
    "    for review, category in zip(reviews, categories):\n",
    "        aspects = aspect_detector.detect_aspects(review, category)\n",
    "        print(f\"\\nReview: {review}\")\n",
    "        print(f\"Category: {category}\")\n",
    "        print(f\"Detected aspects: {aspects}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d5f0e6b4-e040-40ae-b223-a28cee1adbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to C:\\Users\\Manvi\n",
      "[nltk_data]     Bhala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e69e0b23-0d0a-41eb-a69a-946ec76bc348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 reviews for testing\n",
      "Using device: cpu\n",
      "Emotion distribution:\n",
      "  joy: 65 (65.0%)\n",
      "  anger: 17 (17.0%)\n",
      "  sadness: 11 (11.0%)\n",
      "  fear: 3 (3.0%)\n",
      "  love: 2 (2.0%)\n",
      "  surprise: 2 (2.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Bhala\\AppData\\Local\\Temp\\ipykernel_3816\\1807103162.py:211: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df['hour'] = df['timestamp'].dt.floor('H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting real-time sentiment tracking...\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  joy: 1 (100.0%)\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  joy: 1 (100.0%)\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  anger: 1 (100.0%)\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  joy: 1 (100.0%)\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  sadness: 1 (100.0%)\n",
      "Processing batch of 1 reviews...\n",
      "Emotion distribution in this batch:\n",
      "  joy: 1 (100.0%)\n",
      "Completed 30 seconds of real-time tracking\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze()\n",
    "        }\n",
    "\n",
    "class RealTimeSentimentTracker:\n",
    "    def __init__(self, model_name=\"bhadresh-savani/distilbert-base-uncased-emotion\"):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load EmoBERTa model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name).to(self.device)\n",
    "        \n",
    "        # Emotion labels\n",
    "        self.emotions = ['sadness', 'joy', 'love', 'anger', 'fear', 'surprise']\n",
    "        \n",
    "        # Storage for real-time tracking\n",
    "        self.sentiment_history = {\n",
    "            'timestamp': [],\n",
    "            'category': [],\n",
    "            'product_id': [],\n",
    "            'emotion': [],\n",
    "            'score': []\n",
    "        }\n",
    "        \n",
    "        # Category display names\n",
    "        self.category_display_names = {\n",
    "            'All Beauty': 'Beauty',\n",
    "            'Health and Personal Care': 'Health',\n",
    "            'Appliances': 'Appliances',\n",
    "            'Video_Games': 'Games'\n",
    "        }\n",
    "    \n",
    "    def analyze_batch(self, texts, categories=None, product_ids=None, timestamps=None):\n",
    "        \"\"\"Analyze emotions in a batch of texts\"\"\"\n",
    "        if categories is None:\n",
    "            categories = ['unknown'] * len(texts)\n",
    "        \n",
    "        if product_ids is None:\n",
    "            product_ids = ['unknown'] * len(texts)\n",
    "            \n",
    "        if timestamps is None:\n",
    "            timestamps = [datetime.now()] * len(texts)\n",
    "            \n",
    "        dataset = EmotionDataset(texts, self.tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=16)\n",
    "        \n",
    "        results = []\n",
    "        self.model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                predictions = torch.softmax(outputs.logits, dim=1)\n",
    "                \n",
    "                for pred in predictions:\n",
    "                    emotion_scores = {emotion: score.item() for emotion, score in zip(self.emotions, pred)}\n",
    "                    results.append(emotion_scores)\n",
    "        \n",
    "        # Update sentiment history\n",
    "        for i, emotion_scores in enumerate(results):\n",
    "            for emotion, score in emotion_scores.items():\n",
    "                self.sentiment_history['timestamp'].append(timestamps[i])\n",
    "                self.sentiment_history['category'].append(categories[i])\n",
    "                self.sentiment_history['product_id'].append(product_ids[i])\n",
    "                self.sentiment_history['emotion'].append(emotion)\n",
    "                self.sentiment_history['score'].append(score)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def process_stream(self, review_stream, batch_size=32, interval_seconds=5, max_duration_seconds=60):\n",
    "        \"\"\"Process a stream of reviews in real-time\"\"\"\n",
    "        print(\"Starting real-time sentiment tracking...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        end_time = start_time + max_duration_seconds\n",
    "        \n",
    "        while time.time() < end_time:\n",
    "            # Collect reviews from the stream\n",
    "            batch_texts = []\n",
    "            batch_categories = []\n",
    "            batch_product_ids = []\n",
    "            batch_timestamps = []\n",
    "            \n",
    "            for _ in range(batch_size):\n",
    "                if review_stream.has_new():\n",
    "                    review = review_stream.get_next()\n",
    "                    batch_texts.append(review['text'])\n",
    "                    batch_categories.append(review['category'])\n",
    "                    batch_product_ids.append(review.get('product_id', 'unknown'))\n",
    "                    batch_timestamps.append(review.get('timestamp', datetime.now()))\n",
    "                \n",
    "                if len(batch_texts) == 0:\n",
    "                    time.sleep(1)  # Wait for new reviews\n",
    "                    continue\n",
    "            \n",
    "            # Process the batch\n",
    "            if batch_texts:\n",
    "                print(f\"Processing batch of {len(batch_texts)} reviews...\")\n",
    "                results = self.analyze_batch(\n",
    "                    batch_texts, \n",
    "                    batch_categories, \n",
    "                    batch_product_ids, \n",
    "                    batch_timestamps\n",
    "                )\n",
    "                \n",
    "                # Print summary\n",
    "                emotions_summary = {}\n",
    "                for emotion_scores in results:\n",
    "                    top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "                    emotions_summary[top_emotion] = emotions_summary.get(top_emotion, 0) + 1\n",
    "                \n",
    "                print(\"Emotion distribution in this batch:\")\n",
    "                for emotion, count in emotions_summary.items():\n",
    "                    print(f\"  {emotion}: {count} ({count/len(results)*100:.1f}%)\")\n",
    "            \n",
    "            # Wait for the next interval\n",
    "            time.sleep(interval_seconds)\n",
    "        \n",
    "        print(f\"Completed {max_duration_seconds} seconds of real-time tracking\")\n",
    "    \n",
    "    def get_sentiment_trends(self, category=None, product_id=None, time_window=None):\n",
    "        \"\"\"Get sentiment trends for analysis\"\"\"\n",
    "        df = pd.DataFrame(self.sentiment_history)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Filter by category if specified\n",
    "        if category:\n",
    "            df = df[df['category'] == category]\n",
    "        \n",
    "        # Filter by product if specified\n",
    "        if product_id:\n",
    "            df = df[df['product_id'] == product_id]\n",
    "        \n",
    "        # Filter by time window if specified\n",
    "        if time_window:\n",
    "            cutoff_time = datetime.now() - time_window\n",
    "            df = df[df['timestamp'] >= cutoff_time]\n",
    "        \n",
    "        # Group by emotion and calculate average score\n",
    "        trends = df.groupby(['emotion'])['score'].mean().reset_index()\n",
    "        trends = trends.sort_values('score', ascending=False)\n",
    "        \n",
    "        return trends\n",
    "    \n",
    "    def visualize_emotion_trends(self, category=None, product_id=None, time_window=None):\n",
    "        \"\"\"Visualize emotion trends over time\"\"\"\n",
    "        df = pd.DataFrame(self.sentiment_history)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"No data available for visualization\")\n",
    "            return None\n",
    "        \n",
    "        # Filter by category if specified\n",
    "        if category:\n",
    "            df = df[df['category'] == category]\n",
    "            title_suffix = f\" for {self.category_display_names.get(category, category)}\"\n",
    "        else:\n",
    "            title_suffix = \" across all categories\"\n",
    "        \n",
    "        # Filter by product if specified\n",
    "        if product_id:\n",
    "            df = df[df['product_id'] == product_id]\n",
    "            title_suffix = f\" for product {product_id}\"\n",
    "        \n",
    "        # Filter by time window if specified\n",
    "        if time_window:\n",
    "            cutoff_time = datetime.now() - time_window\n",
    "            df = df[df['timestamp'] >= cutoff_time]\n",
    "        \n",
    "        # Convert timestamp to datetime if it's not already\n",
    "        if not pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        # Group by timestamp (hourly) and emotion\n",
    "        df['hour'] = df['timestamp'].dt.floor('H')\n",
    "        emotion_over_time = df.groupby(['hour', 'emotion'])['score'].mean().reset_index()\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        for emotion in self.emotions:\n",
    "            emotion_data = emotion_over_time[emotion_over_time['emotion'] == emotion]\n",
    "            if not emotion_data.empty:\n",
    "                plt.plot(emotion_data['hour'], emotion_data['score'], marker='o', label=emotion)\n",
    "        \n",
    "        plt.title(f\"Emotion Trends{title_suffix}\")\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Average Score\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = \"emotion_trends.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        return filename\n",
    "    \n",
    "    def visualize_emotion_distribution(self, category=None):\n",
    "        \"\"\"Visualize the distribution of emotions\"\"\"\n",
    "        df = pd.DataFrame(self.sentiment_history)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            print(\"No data available for visualization\")\n",
    "            return None\n",
    "        \n",
    "        # Filter by category if specified\n",
    "        if category:\n",
    "            df = df[df['category'] == category]\n",
    "            title_suffix = f\" for {self.category_display_names.get(category, category)}\" \n",
    "            df = df[df['category'] == category]\n",
    "            title_suffix = f\" for {self.category_display_names.get(category, category)}\"\n",
    "        else:\n",
    "            title_suffix = \" across all categories\"\n",
    "        \n",
    "        # Get the top emotion for each review\n",
    "        df_top_emotions = df.loc[df.groupby(['timestamp', 'category', 'product_id'])['score'].idxmax()]\n",
    "        \n",
    "        # Count the occurrences of each emotion\n",
    "        emotion_counts = df_top_emotions['emotion'].value_counts().reset_index()\n",
    "        emotion_counts.columns = ['emotion', 'count']\n",
    "        \n",
    "        # Create the plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x='emotion', y='count', data=emotion_counts)\n",
    "        \n",
    "        plt.title(f\"Emotion Distribution{title_suffix}\")\n",
    "        plt.xlabel(\"Emotion\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        filename = \"emotion_distribution.png\"\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        \n",
    "        return filename\n",
    "\n",
    "# Example of a simple review stream simulator for testing\n",
    "class ReviewStreamSimulator:\n",
    "    def __init__(self, reviews_df, rate=5):\n",
    "        \"\"\"\n",
    "        Simulate a stream of reviews\n",
    "        \n",
    "        Args:\n",
    "            reviews_df: DataFrame with reviews\n",
    "            rate: Average number of reviews per second\n",
    "        \"\"\"\n",
    "        self.reviews = reviews_df\n",
    "        self.rate = rate\n",
    "        self.index = 0\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "    def has_new(self):\n",
    "        \"\"\"Check if new reviews are available\"\"\"\n",
    "        if self.index >= len(self.reviews):\n",
    "            return False\n",
    "            \n",
    "        current_time = time.time()\n",
    "        time_diff = current_time - self.last_time\n",
    "        expected_reviews = time_diff * self.rate\n",
    "        \n",
    "        return expected_reviews >= 1\n",
    "    \n",
    "    def get_next(self):\n",
    "        \"\"\"Get the next review\"\"\"\n",
    "        if self.index >= len(self.reviews):\n",
    "            return None\n",
    "            \n",
    "        review = {\n",
    "            'text': self.reviews.iloc[self.index]['reviewText'],\n",
    "            'category': self.reviews.iloc[self.index]['category'],\n",
    "            'product_id': self.reviews.iloc[self.index].get('asin', 'unknown'),\n",
    "            'timestamp': self.reviews.iloc[self.index].get('datetime', datetime.now())\n",
    "        }\n",
    "        \n",
    "        self.index += 1\n",
    "        self.last_time = time.time()\n",
    "        \n",
    "        return review\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load sample data\n",
    "    try:\n",
    "        reviews_df = pd.read_parquet(\"filtered_amazon_reviews.parquet\")\n",
    "        print(f\"Loaded {len(reviews_df)} reviews for testing\")\n",
    "        \n",
    "        # Create a review stream simulator\n",
    "        stream = ReviewStreamSimulator(reviews_df, rate=10)\n",
    "        \n",
    "        # Create and start the sentiment tracker\n",
    "        tracker = RealTimeSentimentTracker()\n",
    "        \n",
    "        # Process a batch of reviews\n",
    "        sample_reviews = reviews_df['reviewText'].tolist()[:100]\n",
    "        sample_categories = reviews_df['category'].tolist()[:100]\n",
    "        sample_product_ids = reviews_df['asin'].tolist()[:100]\n",
    "        \n",
    "        results = tracker.analyze_batch(sample_reviews, sample_categories, sample_product_ids)\n",
    "        \n",
    "        # Print summary\n",
    "        emotions_summary = {}\n",
    "        for emotion_scores in results:\n",
    "            top_emotion = max(emotion_scores, key=emotion_scores.get)\n",
    "            emotions_summary[top_emotion] = emotions_summary.get(top_emotion, 0) + 1\n",
    "        \n",
    "        print(\"Emotion distribution:\")\n",
    "        for emotion, count in emotions_summary.items():\n",
    "            print(f\"  {emotion}: {count} ({count/len(results)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualize results\n",
    "        tracker.visualize_emotion_distribution()\n",
    "        tracker.visualize_emotion_trends()\n",
    "        \n",
    "        # Process stream for a short time\n",
    "        tracker.process_stream(stream, max_duration_seconds=30)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run data_loader.py first to create the filtered dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "479f8119-af0d-4ed6-9d9c-77bcf622a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 reviews for analysis\n",
      "Detected 5 potential fake reviews out of 1000 (0.5%)\n",
      "Network visualization saved as review_network.png\n",
      "Analysis results:\n",
      "  Total reviews: 1000\n",
      "  Fake reviews: 5 (0.5%)\n",
      "  Rating distribution chart saved as rating_distribution.png\n",
      "  Category distribution chart saved as category_distribution.png\n",
      "  Fake review percentage by category:\n",
      "    Video Games: 0.9%\n",
      "    Appliances: 0.0%\n",
      "    Health and Personal Care: 0.0%\n",
      "    All Beauty: 0.0%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "class DomainAdapter:\n",
    "    \"\"\"\n",
    "    The DomainAdapter class serves as an abstract base class for adapting data\n",
    "    from various domain-specific formats into a standardized format suitable\n",
    "    for further processing, such as fake review detection.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the DomainAdapter. This base class doesn't require any\n",
    "        specific initialization parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def load_data(self, file_path):\n",
    "        \"\"\"\n",
    "        Abstract method to load data from a file. Subclasses must implement\n",
    "        this method to handle the specifics of their data format.\n",
    "\n",
    "        Args:\n",
    "            file_path (str): The path to the data file.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing the loaded data.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If the method is not implemented in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement load_data method\")\n",
    "\n",
    "    def transform_data(self, df):\n",
    "        \"\"\"\n",
    "        Abstract method to transform the loaded data into a standardized format.\n",
    "        Subclasses must implement this method to handle the specifics of their\n",
    "        data format.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The DataFrame to transform.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame containing the transformed data.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: If the method is not implemented in a subclass.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement transform_data method\")\n",
    "\n",
    "    def standardize_columns(self, df, column_mapping):\n",
    "        \"\"\"\n",
    "        Standardizes column names in the DataFrame based on a provided mapping.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The DataFrame to standardize.\n",
    "            column_mapping (dict): A dictionary mapping original column names\n",
    "                                   to standardized column names.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with standardized column names.\n",
    "        \"\"\"\n",
    "        return df.rename(columns=column_mapping)\n",
    "\n",
    "    def handle_missing_values(self, df, strategy='drop'):\n",
    "        \"\"\"\n",
    "        Handles missing values in the DataFrame based on a specified strategy.\n",
    "\n",
    "        Args:\n",
    "            df (pandas.DataFrame): The DataFrame to handle missing values in.\n",
    "            strategy (str): The strategy for handling missing values.\n",
    "                            'drop' to drop rows with any missing values.\n",
    "                            'fill' to fill missing values with a default value.\n",
    "\n",
    "        Returns:\n",
    "            pandas.DataFrame: A DataFrame with missing values handled.\n",
    "        \"\"\"\n",
    "        if strategy == 'drop':\n",
    "            df = df.dropna()\n",
    "        elif strategy == 'fill':\n",
    "            # Fill missing values with a default value (e.g., empty string)\n",
    "            df = df.fillna('')\n",
    "        return df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class DomainAdapter:\n",
    "    def __init__(self, base_model_name=\"distilbert-base-uncased\"):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load base model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        self.base_model = AutoModel.from_pretrained(base_model_name).to(self.device)\n",
    "        \n",
    "        # Domain-specific classifier head\n",
    "        self.domain_classifiers = {}\n",
    "        \n",
    "        # Category-specific adjustments\n",
    "        self.category_adjustments = {\n",
    "            \"All Beauty\": 1.0,  # No adjustment\n",
    "            \"Health and Personal Care\": 1.0,  # No adjustment\n",
    "            \"Appliances\": 0.95,  # Slightly reduce suspicion for appliance reviews\n",
    "            \"Video_Games\": 0.9   # Reduce suspicion more for video game reviews (more emotional)\n",
    "        }\n",
    "    \n",
    "    def create_domain_classifier(self, domain, num_classes):\n",
    "        \"\"\"Create a classifier head for a specific domain\"\"\"\n",
    "        # Get the hidden size of the base model\n",
    "        hidden_size = self.base_model.config.hidden_size\n",
    "        \n",
    "        # Create a simple classifier\n",
    "        classifier = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size // 2, num_classes)\n",
    "        ).to(self.device)\n",
    "        \n",
    "        self.domain_classifiers[domain] = classifier\n",
    "        return classifier\n",
    "    \n",
    "    def train_domain_model(self, domain, texts, labels, num_classes, epochs=3, batch_size=16):\n",
    "        \"\"\"Train a domain-specific model\"\"\"\n",
    "        # Create dataset\n",
    "        train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "            texts, labels, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        train_dataset = ReviewDataset(train_texts, train_labels, self.tokenizer)\n",
    "        val_dataset = ReviewDataset(val_texts, val_labels, self.tokenizer)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Create or get domain classifier\n",
    "        if domain not in self.domain_classifiers:\n",
    "            classifier = self.create_domain_classifier(domain, num_classes)\n",
    "        else:\n",
    "            classifier = self.domain_classifiers[domain]\n",
    "        \n",
    "        # Optimizer\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            list(self.base_model.parameters()) + list(classifier.parameters()),\n",
    "            lr=5e-5\n",
    "        )\n",
    "        \n",
    "        # Training loop\n",
    "        best_accuracy = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Training\n",
    "            self.base_model.train()\n",
    "            classifier.train()\n",
    "            train_loss = 0\n",
    "            \n",
    "            for batch in train_loader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                hidden_states = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "                logits = classifier(hidden_states)\n",
    "                \n",
    "                # Loss\n",
    "                loss = F.cross_entropy(logits, labels)\n",
    "                train_loss += loss.item()\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            train_loss /= len(train_loader)\n",
    "            \n",
    "            # Validation\n",
    "            self.base_model.eval()\n",
    "            classifier.eval()\n",
    "            val_loss = 0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch in val_loader:\n",
    "                    input_ids = batch['input_ids'].to(self.device)\n",
    "                    attention_mask = batch['attention_mask'].to(self.device)\n",
    "                    labels = batch['label'].to(self.device)\n",
    "                    \n",
    "                    # Forward pass\n",
    "                    outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                    hidden_states = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "                    logits = classifier(hidden_states)\n",
    "                    \n",
    "                    # Loss\n",
    "                    loss = F.cross_entropy(logits, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    \n",
    "                    # Accuracy\n",
    "                    _, predicted = torch.max(logits, 1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            val_loss /= len(val_loader)\n",
    "            accuracy = correct / total\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "            \n",
    "            if accuracy > best_accuracy:\n",
    "                best_accuracy = accuracy\n",
    "                # Save model if needed\n",
    "        \n",
    "        print(f\"Training completed for domain: {domain}, Best Accuracy: {best_accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, domain, texts, batch_size=16):\n",
    "        \"\"\"Make predictions using a domain-specific model\"\"\"\n",
    "        if domain not in self.domain_classifiers:\n",
    "            raise ValueError(f\"No model trained for domain: {domain}\")\n",
    "        \n",
    "        classifier = self.domain_classifiers[domain]\n",
    "        dataset = ReviewDataset(texts, [0] * len(texts), self.tokenizer)  # Dummy labels\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "        \n",
    "        predictions = []\n",
    "        \n",
    "        self.base_model.eval()\n",
    "        classifier.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                hidden_states = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "                logits = classifier(hidden_states)\n",
    "                \n",
    "                # Get predictions\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "                batch_preds = probs.cpu().numpy()\n",
    "                predictions.extend(batch_preds)\n",
    "        \n",
    "        return np.array(predictions)\n",
    "\n",
    "class FakeReviewDetector:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        # Load a pre-trained model for review embeddings\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "        self.embedding_model = AutoModel.from_pretrained(\"distilbert-base-uncased\").to(self.device)\n",
    "        \n",
    "        # GNN for fake review detection\n",
    "        self.gnn_model = None\n",
    "        \n",
    "        # Category-specific adjustments\n",
    "        self.category_adjustments = {\n",
    "            \"All Beauty\": 1.0,  # No adjustment\n",
    "            \"Health and Personal Care\": 1.0,  # No adjustment\n",
    "            \"Appliances\": 0.95,  # Slightly reduce suspicion for appliance reviews\n",
    "            \"Video_Games\": 0.9   # Reduce suspicion more for video game reviews (more emotional)\n",
    "        }\n",
    "    \n",
    "    def extract_product_details(self, details_str):\n",
    "        \"\"\"Extract product details from the JSON string\"\"\"\n",
    "        try:\n",
    "            if details_str and isinstance(details_str, str):\n",
    "                details = json.loads(details_str)\n",
    "                return details\n",
    "            return {}\n",
    "        except:\n",
    "            return {}\n",
    "    \n",
    "    def build_review_graph(self, reviews_df):\n",
    "        \"\"\"Build a heterogeneous graph with reviews, users, and products\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add nodes for reviews\n",
    "        for i, row in reviews_df.iterrows():\n",
    "            G.add_node(f\"review_{i}\", type=\"review\", text=row['reviewText'], rating=row['rating'])\n",
    "        \n",
    "        # Add nodes for users\n",
    "        user_ids = set(reviews_df['user_id'])\n",
    "        for user_id in user_ids:\n",
    "            G.add_node(f\"user_{user_id}\", type=\"user\")\n",
    "        \n",
    "        # Add nodes for products\n",
    "        product_ids = set(reviews_df['asin'])\n",
    "        for product_id in product_ids:\n",
    "            G.add_node(f\"product_{product_id}\", type=\"product\")\n",
    "        \n",
    "        # Add edges between reviews and users/products\n",
    "        for i, row in reviews_df.iterrows():\n",
    "            G.add_edge(f\"review_{i}\", f\"user_{row['user_id']}\")\n",
    "            G.add_edge(f\"review_{i}\", f\"product_{row['asin']}\")\n",
    "        \n",
    "        # Add edges between users who reviewed the same product\n",
    "        user_product_dict = {}\n",
    "        for _, row in reviews_df.iterrows():\n",
    "            user_id = row['user_id']\n",
    "            product_id = row['asin']\n",
    "            \n",
    "            if user_id not in user_product_dict:\n",
    "                user_product_dict[user_id] = set()\n",
    "            user_product_dict[user_id].add(product_id)\n",
    "        \n",
    "        for user1 in user_product_dict:\n",
    "            for user2 in user_product_dict:\n",
    "                if user1 != user2:\n",
    "                    common_products = user_product_dict[user1].intersection(user_product_dict[user2])\n",
    "                    if len(common_products) > 0:\n",
    "                        G.add_edge(f\"user_{user1}\", f\"user_{user2}\", weight=len(common_products))\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def get_review_embeddings(self, reviews, batch_size=16):\n",
    "        \"\"\"Get embeddings for reviews using the pre-trained model\"\"\"\n",
    "        embeddings = []\n",
    "        \n",
    "        self.embedding_model.eval()\n",
    "        \n",
    "        for i in range(0, len(reviews), batch_size):\n",
    "            batch_reviews = reviews[i:i+batch_size]\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                batch_reviews,\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=128,\n",
    "                return_tensors=\"pt\"\n",
    "            ).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = self.embedding_model(**inputs)\n",
    "                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # CLS token\n",
    "                embeddings.extend(batch_embeddings)\n",
    "        \n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    def detect_fake_reviews(self, reviews_df):\n",
    "        \"\"\"Detect fake reviews using graph-based approach\"\"\"\n",
    "        # Extract necessary data\n",
    "        reviews = reviews_df['reviewText'].tolist()\n",
    "        user_ids = reviews_df['user_id'].tolist()\n",
    "        product_ids = reviews_df['asin'].tolist()\n",
    "        ratings = reviews_df['rating'].tolist()\n",
    "        categories = reviews_df['category'].tolist() if 'category' in reviews_df.columns else ['unknown'] * len(reviews)\n",
    "        \n",
    "        # Get timestamps if available\n",
    "        if 'datetime' in reviews_df.columns:\n",
    "            timestamps = reviews_df['datetime'].tolist()\n",
    "        elif 'timestamp' in reviews_df.columns:\n",
    "            timestamps = [datetime.fromtimestamp(ts/1000) if isinstance(ts, (int, float)) else ts \n",
    "                         for ts in reviews_df['timestamp'].tolist()]\n",
    "        else:\n",
    "            timestamps = [None] * len(reviews)\n",
    "        \n",
    "        # Build the graph\n",
    "        G = self.build_review_graph(reviews_df)\n",
    "        \n",
    "        # Get review embeddings\n",
    "        review_embeddings = self.get_review_embeddings(reviews)\n",
    "        \n",
    "        # Calculate review similarity\n",
    "        from sklearn.metrics.pairwise import cosine_similarity\n",
    "        similarity_matrix = cosine_similarity(review_embeddings)\n",
    "        \n",
    "        # Identify suspicious patterns\n",
    "        suspicious_scores = np.zeros(len(reviews))\n",
    "        \n",
    "        # 1. Check for users with multiple similar reviews\n",
    "        user_reviews = {}\n",
    "        for i, user_id in enumerate(user_ids):\n",
    "            if user_id not in user_reviews:\n",
    "                user_reviews[user_id] = []\n",
    "            user_reviews[user_id].append(i)\n",
    "        \n",
    "        for user_id, review_indices in user_reviews.items():\n",
    "            if len(review_indices) > 1:\n",
    "                # Check similarity between reviews by the same user\n",
    "                for i in range(len(review_indices)):\n",
    "                    for j in range(i+1, len(review_indices)):\n",
    "                        idx1, idx2 = review_indices[i], review_indices[j]\n",
    "                        if similarity_matrix[idx1, idx2] > 0.8:  # High similarity threshold\n",
    "                            suspicious_scores[idx1] += 0.5\n",
    "                            suspicious_scores[idx2] += 0.5\n",
    "        \n",
    "        # 2. Check for burst patterns (many reviews in a short time)\n",
    "        if timestamps[0] is not None:\n",
    "            # Group reviews by product\n",
    "            product_reviews = {}\n",
    "            for i, product_id in enumerate(product_ids):\n",
    "                if product_id not in product_reviews:\n",
    "                    product_reviews[product_id] = []\n",
    "                product_reviews[product_id].append(i)\n",
    "            \n",
    "            # Check for burst patterns\n",
    "            for product_id, review_indices in product_reviews.items():\n",
    "                if len(review_indices) > 1:\n",
    "                    # Sort by timestamp\n",
    "                    sorted_indices = sorted(review_indices, key=lambda i: timestamps[i])\n",
    "                    \n",
    "                    # Check for reviews in quick succession\n",
    "                    for i in range(len(sorted_indices)-1):\n",
    "                        idx1, idx2 = sorted_indices[i], sorted_indices[i+1]\n",
    "                        time_diff = (timestamps[idx2] - timestamps[idx1]).total_seconds()\n",
    "                        \n",
    "                        # If reviews are less than 1 hour apart\n",
    "                        if time_diff < 3600:\n",
    "                            suspicious_scores[idx1] += 0.3\n",
    "                            suspicious_scores[idx2] += 0.3\n",
    "        \n",
    "        # 3. Check for network patterns\n",
    "        # Users who are connected and have similar review patterns\n",
    "        for user1 in user_reviews:\n",
    "            for user2 in user_reviews:\n",
    "                if user1 != user2 and G.has_edge(f\"user_{user1}\", f\"user_{user2}\"):\n",
    "                    # Check if they have similar review patterns\n",
    "                    reviews1 = user_reviews[user1]\n",
    "                    reviews2 = user_reviews[user2]\n",
    "                    \n",
    "                    for idx1 in reviews1:\n",
    "                        for idx2 in reviews2:\n",
    "                            if similarity_matrix[idx1, idx2] > 0.7:  # Similarity threshold\n",
    "                                suspicious_scores[idx1] += 0.3\n",
    "                                suspicious_scores[idx2] += 0.3\n",
    "        \n",
    "        # 4. Check for extreme ratings\n",
    "        for i, rating in enumerate(ratings):\n",
    "            # Extreme ratings (1 or 5) are slightly more suspicious\n",
    "            if rating == 1 or rating == 5:\n",
    "                suspicious_scores[i] += 0.1\n",
    "        \n",
    "        # 5. Apply category-specific adjustments\n",
    "        for i, category in enumerate(categories):\n",
    "            if category in self.category_adjustments:\n",
    "                suspicious_scores[i] *= self.category_adjustments[category]\n",
    "        \n",
    "        # Normalize scores\n",
    "        if suspicious_scores.max() > 0:\n",
    "            suspicious_scores = suspicious_scores / suspicious_scores.max()\n",
    "        \n",
    "        # Classify reviews as fake or genuine (threshold = 0.7)\n",
    "        threshold = 0.7\n",
    "        fake_labels = (suspicious_scores > threshold).astype(int)\n",
    "        \n",
    "        # Add results to the dataframe\n",
    "        result_df = reviews_df.copy()\n",
    "        result_df['suspicious_score'] = suspicious_scores\n",
    "        result_df['is_fake'] = fake_labels\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "    def visualize_review_network(self, reviews_df, output_file=\"review_network.png\"):\n",
    "        \"\"\"Visualize the review network with fake reviews highlighted\"\"\"\n",
    "        # Ensure we have fake labels\n",
    "        if 'is_fake' not in reviews_df.columns:\n",
    "            reviews_df = self.detect_fake_reviews(reviews_df)\n",
    "        \n",
    "        # Build the graph\n",
    "        G = self.build_review_graph(reviews_df)\n",
    "        \n",
    "        # Create a plot\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Define node colors based on type and fake status\n",
    "        node_colors = []\n",
    "        for node in G.nodes():\n",
    "            if node.startswith('review_'):\n",
    "                review_idx = int(node.split('_')[1])\n",
    "                if review_idx < len(reviews_df) and reviews_df.iloc[review_idx]['is_fake'] == 1:\n",
    "                    node_colors.append('red')  # Fake reviews\n",
    "                else:\n",
    "                    node_colors.append('green')  # Genuine reviews\n",
    "            elif node.startswith('user_'):\n",
    "                node_colors.append('blue')  # Users\n",
    "            else:\n",
    "                node_colors.append('orange')  # Products\n",
    "        \n",
    "        # Draw the graph\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, alpha=0.8)\n",
    "        nx.draw_networkx_edges(G, pos, alpha=0.2)\n",
    "        \n",
    "        # Add labels for important nodes\n",
    "        labels = {}\n",
    "        for node in G.nodes():\n",
    "            if node.startswith('user_') or node.startswith('product_'):\n",
    "                labels[node] = node\n",
    "        nx.draw_networkx_labels(G, pos, labels=labels, font_size=8)\n",
    "        \n",
    "        plt.title(\"Review Network (Red: Fake Reviews, Green: Genuine Reviews)\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(output_file)\n",
    "        plt.close()\n",
    "        \n",
    "        return output_file\n",
    "    \n",
    "    def analyze_fake_review_patterns(self, reviews_df):\n",
    "        \"\"\"Analyze patterns in fake reviews\"\"\"\n",
    "        # Ensure we have fake labels\n",
    "        if 'is_fake' not in reviews_df.columns:\n",
    "            reviews_df = self.detect_fake_reviews(reviews_df)\n",
    "        \n",
    "        # Get fake and genuine reviews\n",
    "        fake_reviews = reviews_df[reviews_df['is_fake'] == 1]\n",
    "        genuine_reviews = reviews_df[reviews_df['is_fake'] == 0]\n",
    "        \n",
    "        # Calculate statistics\n",
    "        stats = {\n",
    "            'total_reviews': len(reviews_df),\n",
    "            'fake_reviews': len(fake_reviews),\n",
    "            'genuine_reviews': len(genuine_reviews),\n",
    "            'fake_percentage': len(fake_reviews) / len(reviews_df) * 100 if len(reviews_df) > 0 else 0\n",
    "        }\n",
    "        \n",
    "        # Rating distribution\n",
    "        fake_rating_dist = fake_reviews['rating'].value_counts().sort_index()\n",
    "        genuine_rating_dist = genuine_reviews['rating'].value_counts().sort_index()\n",
    "        \n",
    "        # Visualize rating distribution\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        \n",
    "        # Create bar positions\n",
    "        bar_width = 0.35\n",
    "        index = np.arange(5)  # 5 possible ratings (1-5)\n",
    "        \n",
    "        # Get counts for each rating\n",
    "        fake_counts = [fake_rating_dist.get(i+1, 0) for i in range(5)]\n",
    "        genuine_counts = [genuine_rating_dist.get(i+1, 0) for i in range(5)]\n",
    "        \n",
    "        # Plot bars\n",
    "        plt.bar(index, fake_counts, bar_width, label='Fake Reviews', color='red', alpha=0.7)\n",
    "        plt.bar(index + bar_width, genuine_counts, bar_width, label='Genuine Reviews', color='green', alpha=0.7)\n",
    "        \n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title('Rating Distribution: Fake vs. Genuine Reviews')\n",
    "        plt.xticks(index + bar_width/2, ['1', '2', '3', '4', '5'])\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the plot\n",
    "        rating_dist_file = \"rating_distribution.png\"\n",
    "        plt.savefig(rating_dist_file)\n",
    "        plt.close()\n",
    "        \n",
    "        # Category distribution\n",
    "        if 'category' in reviews_df.columns:\n",
    "            fake_category_dist = fake_reviews['category'].value_counts()\n",
    "            genuine_category_dist = genuine_reviews['category'].value_counts()\n",
    "            \n",
    "            # Calculate fake review percentage by category\n",
    "            category_stats = []\n",
    "            for category in reviews_df['category'].unique():\n",
    "                category_reviews = reviews_df[reviews_df['category'] == category]\n",
    "                category_fake = category_reviews[category_reviews['is_fake'] == 1]\n",
    "                \n",
    "                category_stats.append({\n",
    "                    'category': category,\n",
    "                    'total': len(category_reviews),\n",
    "                    'fake': len(category_fake),\n",
    "                    'fake_percentage': len(category_fake) / len(category_reviews) * 100\n",
    "                })\n",
    "            \n",
    "            # Sort by fake percentage\n",
    "            category_stats = sorted(category_stats, key=lambda x: x['fake_percentage'], reverse=True)\n",
    "            \n",
    "            # Visualize category distribution\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            \n",
    "            categories = [stat['category'] for stat in category_stats]\n",
    "            percentages = [stat['fake_percentage'] for stat in category_stats]\n",
    "            \n",
    "            plt.bar(categories, percentages, color='red', alpha=0.7)\n",
    "            plt.xlabel('Category')\n",
    "            plt.ylabel('Fake Review Percentage')\n",
    "            plt.title('Fake Review Percentage by Category')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save the plot\n",
    "            category_dist_file = \"category_distribution.png\"\n",
    "            plt.savefig(category_dist_file)\n",
    "            plt.close()\n",
    "        else:\n",
    "            category_stats = []\n",
    "            category_dist_file = None\n",
    "        \n",
    "        return {\n",
    "            'stats': stats,\n",
    "            'rating_distribution_file': rating_dist_file,\n",
    "            'category_distribution_file': category_dist_file,\n",
    "            'category_stats': category_stats\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load sample data\n",
    "    try:\n",
    "        reviews_df = pd.read_parquet(\"filtered_amazon_reviews.parquet\")\n",
    "        print(f\"Loaded {len(reviews_df)} reviews for analysis\")\n",
    "        \n",
    "        # Create the fake review detector\n",
    "        detector = FakeReviewDetector()\n",
    "        \n",
    "        # Sample a subset for demonstration\n",
    "        sample_df = reviews_df.sample(min(1000, len(reviews_df)), random_state=42)\n",
    "        \n",
    "        # Detect fake reviews\n",
    "        result_df = detector.detect_fake_reviews(sample_df)\n",
    "        \n",
    "        # Count fake reviews\n",
    "        fake_count = result_df['is_fake'].sum()\n",
    "        print(f\"Detected {fake_count} potential fake reviews out of {len(result_df)} ({fake_count/len(result_df)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualize the network\n",
    "        network_file = detector.visualize_review_network(result_df)\n",
    "        print(f\"Network visualization saved as {network_file}\")\n",
    "        \n",
    "        # Analyze fake review patterns\n",
    "        analysis = detector.analyze_fake_review_patterns(result_df)\n",
    "        print(f\"Analysis results:\")\n",
    "        print(f\"  Total reviews: {analysis['stats']['total_reviews']}\")\n",
    "        print(f\"  Fake reviews: {analysis['stats']['fake_reviews']} ({analysis['stats']['fake_percentage']:.1f}%)\")\n",
    "        print(f\"  Rating distribution chart saved as {analysis['rating_distribution_file']}\")\n",
    "        \n",
    "        if analysis['category_distribution_file']:\n",
    "            print(f\"  Category distribution chart saved as {analysis['category_distribution_file']}\")\n",
    "            print(\"  Fake review percentage by category:\")\n",
    "            for stat in analysis['category_stats']:\n",
    "                print(f\"    {stat['category']}: {stat['fake_percentage']:.1f}%\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Please run data_loader.py first to create the filtered dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5aa99ada-eec3-4fe2-bea0-eac4f7118031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "class SelfAdaptiveModel:\n",
    "    def __init__(self, base_model_name=\"distilbert-base-uncased-finetuned-sst-2-english\"):\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        \n",
    "        # Load base sentiment model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "        self.model = AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3ccbd1c-9d22-455c-ad99-6595da0fc795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading NLTK resources (if necessary)...\n",
      "NLTK resources checked/downloaded.\n",
      "Results directory './results' checked/created.\n",
      "\n",
      "===============================================\n",
      "=== Starting Interactive Data Loading Section ===\n",
      "===============================================\n",
      "\n",
      "Attempting to load interactive data from: filtered_amazon_reviews.parquet\n",
      "Successfully loaded 10000 reviews from 'filtered_amazon_reviews.parquet'.\n",
      "\n",
      "Interactive Data loaded/generated successfully:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>category</th>\n",
       "      <th>datetime</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>main_category</th>\n",
       "      <th>product_title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>store</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A fabulous new installment to the series!</td>\n",
       "      <td>I LOVE this game. Really looking forward to ne...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01N3NNPAB</td>\n",
       "      <td>B01GY35HKE</td>\n",
       "      <td>AF4H3UFUUEDLMESMC3SCBPQWUIQA</td>\n",
       "      <td>1494468180000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>2017-05-11 07:33:00.000</td>\n",
       "      <td>I LOVE this game. Really looking forward to ne...</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Mass Effect Andromeda Deluxe - Xbox One</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Electronic Arts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Waterloo: Tabletop Wargaming in the Age of Nap...</td>\n",
       "      <td>My son has been a Warhammer enthusiast for the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1907964177</td>\n",
       "      <td>1907964177</td>\n",
       "      <td>AE4PES27ANXCRHV3NSYDL3Z62ZZA</td>\n",
       "      <td>1325389401000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>2012-01-01 09:13:21.000</td>\n",
       "      <td>My son has been a Warhammer enthusiast for the...</td>\n",
       "      <td>Books</td>\n",
       "      <td>Warhammer Historical: Waterloo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mark A. Latham (Author)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Very easy to instal</td>\n",
       "      <td>Refrigerator</td>\n",
       "      <td>[]</td>\n",
       "      <td>B06VW9HKXF</td>\n",
       "      <td>B06VW9HKXF</td>\n",
       "      <td>AH4ULRXVQOWLDB2DWIVIGK3WFEHQ</td>\n",
       "      <td>1674241249612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2023-01-21 00:30:49.612</td>\n",
       "      <td>Refrigerator</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>GOLDEN ICEPURE DA29-00020B Refrigerator Water ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>GOLDEN ICEPURE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07P3B366X</td>\n",
       "      <td>B07P3B366X</td>\n",
       "      <td>AGJJVKCCOTLJC7AGIFDNP4AECTRA</td>\n",
       "      <td>1680651776518</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>2023-04-05 05:12:56.518</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>LIFETIME WARRANTY WR57X10051 Refrigerator Dual...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>BlueStars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>sucks!</td>\n",
       "      <td>Cheap plastic. Soft case. My son broke in with...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B007PX6MFM</td>\n",
       "      <td>B007PX6MFM</td>\n",
       "      <td>AHB66TQ4PPSZG2EDUUBEGTRZCOCQ</td>\n",
       "      <td>1419384559000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>2014-12-24 06:59:19.000</td>\n",
       "      <td>Cheap plastic. Soft case. My son broke in with...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Insten 28-in-1 Game Card Case Compatible with ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>eForCity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                       review_title  \\\n",
       "0     5.0          A fabulous new installment to the series!   \n",
       "1     5.0  Waterloo: Tabletop Wargaming in the Age of Nap...   \n",
       "2     5.0                                Very easy to instal   \n",
       "3     5.0                                   Easy to install.   \n",
       "4     1.0                                             sucks!   \n",
       "\n",
       "                                                text images        asin  \\\n",
       "0  I LOVE this game. Really looking forward to ne...     []  B01N3NNPAB   \n",
       "1  My son has been a Warhammer enthusiast for the...     []  1907964177   \n",
       "2                                       Refrigerator     []  B06VW9HKXF   \n",
       "3                                   Easy to install.     []  B07P3B366X   \n",
       "4  Cheap plastic. Soft case. My son broke in with...     []  B007PX6MFM   \n",
       "\n",
       "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
       "0  B01GY35HKE  AF4H3UFUUEDLMESMC3SCBPQWUIQA  1494468180000             0   \n",
       "1  1907964177  AE4PES27ANXCRHV3NSYDL3Z62ZZA  1325389401000             1   \n",
       "2  B06VW9HKXF  AH4ULRXVQOWLDB2DWIVIGK3WFEHQ  1674241249612             0   \n",
       "3  B07P3B366X  AGJJVKCCOTLJC7AGIFDNP4AECTRA  1680651776518             0   \n",
       "4  B007PX6MFM  AHB66TQ4PPSZG2EDUUBEGTRZCOCQ  1419384559000             0   \n",
       "\n",
       "   verified_purchase     category                datetime  \\\n",
       "0              False  Video Games 2017-05-11 07:33:00.000   \n",
       "1              False  Video Games 2012-01-01 09:13:21.000   \n",
       "2               True   Appliances 2023-01-21 00:30:49.612   \n",
       "3               True   Appliances 2023-04-05 05:12:56.518   \n",
       "4               True  Video Games 2014-12-24 06:59:19.000   \n",
       "\n",
       "                                          reviewText  \\\n",
       "0  I LOVE this game. Really looking forward to ne...   \n",
       "1  My son has been a Warhammer enthusiast for the...   \n",
       "2                                       Refrigerator   \n",
       "3                                   Easy to install.   \n",
       "4  Cheap plastic. Soft case. My son broke in with...   \n",
       "\n",
       "              main_category  \\\n",
       "0               Video Games   \n",
       "1                     Books   \n",
       "2                Appliances   \n",
       "3  Tools & Home Improvement   \n",
       "4                 Computers   \n",
       "\n",
       "                                       product_title  average_rating  \\\n",
       "0            Mass Effect Andromeda Deluxe - Xbox One             4.3   \n",
       "1                     Warhammer Historical: Waterloo             5.0   \n",
       "2  GOLDEN ICEPURE DA29-00020B Refrigerator Water ...             4.7   \n",
       "3  LIFETIME WARRANTY WR57X10051 Refrigerator Dual...             4.6   \n",
       "4  Insten 28-in-1 Game Card Case Compatible with ...             4.5   \n",
       "\n",
       "                     store  \n",
       "0          Electronic Arts  \n",
       "1  Mark A. Latham (Author)  \n",
       "2           GOLDEN ICEPURE  \n",
       "3                BlueStars  \n",
       "4                 eForCity  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Interactive DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 17 columns):\n",
      " #   Column             Non-Null Count  Dtype         \n",
      "---  ------             --------------  -----         \n",
      " 0   rating             10000 non-null  float64       \n",
      " 1   review_title       10000 non-null  object        \n",
      " 2   text               10000 non-null  object        \n",
      " 3   images             10000 non-null  object        \n",
      " 4   asin               10000 non-null  object        \n",
      " 5   parent_asin        10000 non-null  object        \n",
      " 6   user_id            10000 non-null  object        \n",
      " 7   timestamp          10000 non-null  int64         \n",
      " 8   helpful_vote       10000 non-null  int64         \n",
      " 9   verified_purchase  10000 non-null  bool          \n",
      " 10  category           10000 non-null  object        \n",
      " 11  datetime           10000 non-null  datetime64[ns]\n",
      " 12  reviewText         10000 non-null  object        \n",
      " 13  main_category      9858 non-null   object        \n",
      " 14  product_title      10000 non-null  object        \n",
      " 15  average_rating     10000 non-null  float64       \n",
      " 16  store              9904 non-null   object        \n",
      "dtypes: bool(1), datetime64[ns](1), float64(2), int64(2), object(11)\n",
      "memory usage: 1.2+ MB\n",
      "\n",
      "=== Finished Interactive Data Loading Section ===\n",
      "\n",
      "===================================================\n",
      "=== Starting Interactive Analysis Execution Section ===\n",
      "===================================================\n",
      "\n",
      "Starting interactive analysis in 'batch' mode...\n",
      "\n",
      "★★★ Starting Batch Processing Workflow ★★★\n",
      "\n",
      "--- Training Sentiment Analysis Model ---\n",
      "Cleaning text for sentiment model...\n",
      "Using 9967 reviews for training after cleaning.\n",
      "Vectorizing text (TF-IDF)...\n",
      "Splitting data (80% train, 20% test)...\n",
      "Training Naive Bayes model...\n",
      "Evaluating model...\n",
      "Sentiment model accuracy: 0.7738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.15      0.25       357\n",
      "     neutral       0.00      0.00      0.00       140\n",
      "    positive       0.77      1.00      0.87      1497\n",
      "\n",
      "    accuracy                           0.77      1994\n",
      "   macro avg       0.55      0.38      0.37      1994\n",
      "weighted avg       0.74      0.77      0.70      1994\n",
      "\n",
      "Confusion matrix saved to ./results/sentiment_confusion_matrix.png\n",
      "--- Sentiment Model Training Finished ---\n",
      "Model saved to ./results/model_batch_sentiment.pkl\n",
      "Vectorizer saved to ./results/vectorizer_batch_sentiment.pkl\n",
      "\n",
      "--- Detecting Fake Reviews (Placeholder Logic) ---\n",
      "Flagging reviews with rating 1 or 5 AND text length < 15 chars.\n",
      "Flagged 734 reviews based on placeholder logic.\n",
      "Rating distribution plot saved to ./results/fake_rating_distribution.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Bhala\\AppData\\Local\\Temp\\ipykernel_3816\\3011210197.py:191: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x='is_fake', y='category', data=fake_by_category, palette=\"viridis\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category distribution plot saved to ./results/fake_category_distribution.png\n",
      "--- Fake Review Detection Finished ---\n",
      "\n",
      "--- Analyzing Review Aspects (Placeholder: Nouns) ---\n",
      "Applying placeholder aspect extraction (extracting nouns)...\n",
      "Error during batch POS tagging: \n",
      "**********************************************************************\n",
      "  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n",
      "  Please use the NLTK Downloader to obtain the resource:\n",
      "\n",
      "  \u001b[31m>>> import nltk\n",
      "  >>> nltk.download('averaged_perceptron_tagger_eng')\n",
      "  \u001b[0m\n",
      "  For more information see: https://www.nltk.org/data.html\n",
      "\n",
      "  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n",
      "\n",
      "  Searched in:\n",
      "    - 'C:\\\\Users\\\\Manvi Bhala/nltk_data'\n",
      "    - 'C:\\\\Users\\\\Manvi Bhala\\\\anaconda3\\\\envs\\\\tf_env\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Manvi Bhala\\\\anaconda3\\\\envs\\\\tf_env\\\\share\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Manvi Bhala\\\\anaconda3\\\\envs\\\\tf_env\\\\lib\\\\nltk_data'\n",
      "    - 'C:\\\\Users\\\\Manvi Bhala\\\\AppData\\\\Roaming\\\\nltk_data'\n",
      "    - 'C:\\\\nltk_data'\n",
      "    - 'D:\\\\nltk_data'\n",
      "    - 'E:\\\\nltk_data'\n",
      "**********************************************************************\n",
      ". Falling back to row-by-row (slower).\n",
      "Processed 10000 reviews for aspects.\n",
      "No aspects found.\n",
      "--- Aspect Analysis Finished ---\n",
      "\n",
      "--- Analyzing Emotions (Based on Rating) ---\n",
      "Mapped ratings to emotions.\n",
      "Emotion distribution plot saved to ./results/emotion_distribution.png\n",
      "Analyzing emotion trends over time...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Bhala\\AppData\\Local\\Temp\\ipykernel_3816\\3011210197.py:306: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `y` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(y='emotion', data=df, order=ordered_actual_emotions, palette=\"coolwarm\")\n",
      "C:\\Users\\Manvi Bhala\\AppData\\Local\\Temp\\ipykernel_3816\\3011210197.py:323: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  emotion_trends = df_trends.set_index('datetime').groupby('emotion').resample(resample_period).size().unstack(level=0).fillna(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion trends plot saved to ./results/emotion_trends.png\n",
      "--- Emotion Analysis Finished ---\n",
      "\n",
      "Batch processed data saved to ./results/processed_amazon_reviews_batch.parquet\n",
      "★★★ Batch Processing Completed (Duration: 15.70 seconds) ★★★\n",
      "\n",
      "--- Interactive 'batch' Analysis Finished ---\n",
      "Processed DataFrame head:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>review_title</th>\n",
       "      <th>text</th>\n",
       "      <th>images</th>\n",
       "      <th>asin</th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>helpful_vote</th>\n",
       "      <th>verified_purchase</th>\n",
       "      <th>...</th>\n",
       "      <th>main_category</th>\n",
       "      <th>product_title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>store</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>reviewText_str</th>\n",
       "      <th>is_fake</th>\n",
       "      <th>suspicious_score</th>\n",
       "      <th>detected_aspects</th>\n",
       "      <th>emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.0</td>\n",
       "      <td>A fabulous new installment to the series!</td>\n",
       "      <td>I LOVE this game. Really looking forward to ne...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B01N3NNPAB</td>\n",
       "      <td>B01GY35HKE</td>\n",
       "      <td>AF4H3UFUUEDLMESMC3SCBPQWUIQA</td>\n",
       "      <td>1494468180000</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Mass Effect Andromeda Deluxe - Xbox One</td>\n",
       "      <td>4.3</td>\n",
       "      <td>Electronic Arts</td>\n",
       "      <td>love game really looking forward new content s...</td>\n",
       "      <td>I LOVE this game. Really looking forward to ne...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.142320</td>\n",
       "      <td>[]</td>\n",
       "      <td>delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Waterloo: Tabletop Wargaming in the Age of Nap...</td>\n",
       "      <td>My son has been a Warhammer enthusiast for the...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1907964177</td>\n",
       "      <td>1907964177</td>\n",
       "      <td>AE4PES27ANXCRHV3NSYDL3Z62ZZA</td>\n",
       "      <td>1325389401000</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Books</td>\n",
       "      <td>Warhammer Historical: Waterloo</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Mark A. Latham (Author)</td>\n",
       "      <td>son warhammer enthusiast past years christmas ...</td>\n",
       "      <td>My son has been a Warhammer enthusiast for the...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.177134</td>\n",
       "      <td>[]</td>\n",
       "      <td>delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Very easy to instal</td>\n",
       "      <td>Refrigerator</td>\n",
       "      <td>[]</td>\n",
       "      <td>B06VW9HKXF</td>\n",
       "      <td>B06VW9HKXF</td>\n",
       "      <td>AH4ULRXVQOWLDB2DWIVIGK3WFEHQ</td>\n",
       "      <td>1674241249612</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Appliances</td>\n",
       "      <td>GOLDEN ICEPURE DA29-00020B Refrigerator Water ...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>GOLDEN ICEPURE</td>\n",
       "      <td>refrigerator</td>\n",
       "      <td>Refrigerator</td>\n",
       "      <td>True</td>\n",
       "      <td>0.520588</td>\n",
       "      <td>[]</td>\n",
       "      <td>delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>[]</td>\n",
       "      <td>B07P3B366X</td>\n",
       "      <td>B07P3B366X</td>\n",
       "      <td>AGJJVKCCOTLJC7AGIFDNP4AECTRA</td>\n",
       "      <td>1680651776518</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Tools &amp; Home Improvement</td>\n",
       "      <td>LIFETIME WARRANTY WR57X10051 Refrigerator Dual...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>BlueStars</td>\n",
       "      <td>easy install</td>\n",
       "      <td>Easy to install.</td>\n",
       "      <td>False</td>\n",
       "      <td>0.264731</td>\n",
       "      <td>[]</td>\n",
       "      <td>delighted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>sucks!</td>\n",
       "      <td>Cheap plastic. Soft case. My son broke in with...</td>\n",
       "      <td>[]</td>\n",
       "      <td>B007PX6MFM</td>\n",
       "      <td>B007PX6MFM</td>\n",
       "      <td>AHB66TQ4PPSZG2EDUUBEGTRZCOCQ</td>\n",
       "      <td>1419384559000</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>Computers</td>\n",
       "      <td>Insten 28-in-1 Game Card Case Compatible with ...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>eForCity</td>\n",
       "      <td>cheap plastic soft case son broke week waste m...</td>\n",
       "      <td>Cheap plastic. Soft case. My son broke in with...</td>\n",
       "      <td>False</td>\n",
       "      <td>0.141644</td>\n",
       "      <td>[]</td>\n",
       "      <td>angry</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                       review_title  \\\n",
       "0     5.0          A fabulous new installment to the series!   \n",
       "1     5.0  Waterloo: Tabletop Wargaming in the Age of Nap...   \n",
       "2     5.0                                Very easy to instal   \n",
       "3     5.0                                   Easy to install.   \n",
       "4     1.0                                             sucks!   \n",
       "\n",
       "                                                text images        asin  \\\n",
       "0  I LOVE this game. Really looking forward to ne...     []  B01N3NNPAB   \n",
       "1  My son has been a Warhammer enthusiast for the...     []  1907964177   \n",
       "2                                       Refrigerator     []  B06VW9HKXF   \n",
       "3                                   Easy to install.     []  B07P3B366X   \n",
       "4  Cheap plastic. Soft case. My son broke in with...     []  B007PX6MFM   \n",
       "\n",
       "  parent_asin                       user_id      timestamp  helpful_vote  \\\n",
       "0  B01GY35HKE  AF4H3UFUUEDLMESMC3SCBPQWUIQA  1494468180000             0   \n",
       "1  1907964177  AE4PES27ANXCRHV3NSYDL3Z62ZZA  1325389401000             1   \n",
       "2  B06VW9HKXF  AH4ULRXVQOWLDB2DWIVIGK3WFEHQ  1674241249612             0   \n",
       "3  B07P3B366X  AGJJVKCCOTLJC7AGIFDNP4AECTRA  1680651776518             0   \n",
       "4  B007PX6MFM  AHB66TQ4PPSZG2EDUUBEGTRZCOCQ  1419384559000             0   \n",
       "\n",
       "   verified_purchase  ...             main_category  \\\n",
       "0              False  ...               Video Games   \n",
       "1              False  ...                     Books   \n",
       "2               True  ...                Appliances   \n",
       "3               True  ...  Tools & Home Improvement   \n",
       "4               True  ...                 Computers   \n",
       "\n",
       "                                       product_title average_rating  \\\n",
       "0            Mass Effect Andromeda Deluxe - Xbox One            4.3   \n",
       "1                     Warhammer Historical: Waterloo            5.0   \n",
       "2  GOLDEN ICEPURE DA29-00020B Refrigerator Water ...            4.7   \n",
       "3  LIFETIME WARRANTY WR57X10051 Refrigerator Dual...            4.6   \n",
       "4  Insten 28-in-1 Game Card Case Compatible with ...            4.5   \n",
       "\n",
       "                     store                                       cleaned_text  \\\n",
       "0          Electronic Arts  love game really looking forward new content s...   \n",
       "1  Mark A. Latham (Author)  son warhammer enthusiast past years christmas ...   \n",
       "2           GOLDEN ICEPURE                                       refrigerator   \n",
       "3                BlueStars                                       easy install   \n",
       "4                 eForCity  cheap plastic soft case son broke week waste m...   \n",
       "\n",
       "                                      reviewText_str is_fake suspicious_score  \\\n",
       "0  I LOVE this game. Really looking forward to ne...   False         0.142320   \n",
       "1  My son has been a Warhammer enthusiast for the...   False         0.177134   \n",
       "2                                       Refrigerator    True         0.520588   \n",
       "3                                   Easy to install.   False         0.264731   \n",
       "4  Cheap plastic. Soft case. My son broke in with...   False         0.141644   \n",
       "\n",
       "  detected_aspects    emotion  \n",
       "0               []  delighted  \n",
       "1               []  delighted  \n",
       "2               []  delighted  \n",
       "3               []  delighted  \n",
       "4               []      angry  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Finished Interactive Analysis Execution Section ===\n",
      "===================================================\n",
      "=== End of Cell Execution ===\n",
      "===================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--categories CATEGORIES [CATEGORIES ...]] [--mode {batch,stream}]\n",
      "                             [--sample_size SAMPLE_SIZE] [--input_file INPUT_FILE] [--use_hf]\n",
      "                             [--stream_interval STREAM_INTERVAL]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f C:\\Users\\Manvi Bhala\\AppData\\Roaming\\jupyter\\runtime\\kernel-65983b55-de1f-49a5-a21e-907b1fcb1742.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Manvi Bhala\\anaconda3\\envs\\tf_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Amazon Review Analysis (Single Cell)\n",
    "# This cell contains all imports, function definitions, data loading,\n",
    "# and execution logic for interactive analysis in Jupyter.\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# Section 1: Imports and Setup\n",
    "# =============================================================================\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification # Only needed if using embedding model later\n",
    "# import torch.nn.functional as F # Only needed if using embedding model later\n",
    "import time # Added for stream simulation if needed\n",
    "from IPython.display import display # Use display for better notebook formatting\n",
    "\n",
    "# Import custom modules (remain commented out as files are not provided)\n",
    "# from data_loader import load_amazon_reviews_hf, preprocess_reviews\n",
    "# from gnn_context_model import AspectDetector\n",
    "# from domain_adapter import FakeReviewDetector\n",
    "# from context_aware_sentiment import SelfAdaptiveModel\n",
    "\n",
    "# Download NLTK resources (quiet=True suppresses output if already downloaded)\n",
    "print(\"Downloading NLTK resources (if necessary)...\")\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "# Download tagger needed for placeholder aspect analysis\n",
    "# Download tagger needed for placeholder aspect analysis\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "print(\"NLTK resources checked/downloaded.\")\n",
    "\n",
    "# Ensure results directory exists for saving plots/data\n",
    "os.makedirs('./results', exist_ok=True)\n",
    "print(\"Results directory './results' checked/created.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Section 2: Function Definitions\n",
    "# =============================================================================\n",
    "\n",
    "# --- Text Cleaning ---\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean and preprocess text.\"\"\"\n",
    "    if isinstance(text, str):\n",
    "        # Remove HTML tags\n",
    "        try:\n",
    "            text = BeautifulSoup(text, 'html.parser').get_text()\n",
    "        except Exception:\n",
    "            pass # Handle potential parsing errors if text isn't valid HTML\n",
    "        # Remove non-alphabetic characters and handle extra whitespace\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text.lower())\n",
    "        text = re.sub(r'\\s+', ' ', text).strip() # Consolidate whitespace\n",
    "        # Tokenize\n",
    "        words = text.split()\n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [w for w in words if w not in stop_words and len(w) > 1] # Keep words > 1 char\n",
    "        return ' '.join(words)\n",
    "    return \"\"\n",
    "\n",
    "# --- Sentiment Model Training ---\n",
    "def train_sentiment_model(df):\n",
    "    \"\"\"Train a Naive Bayes model for sentiment analysis.\"\"\"\n",
    "    print(\"\\n--- Training Sentiment Analysis Model ---\")\n",
    "    if 'reviewText' not in df.columns or 'rating' not in df.columns:\n",
    "        print(\"Error: DataFrame must contain 'reviewText' and 'rating' columns.\")\n",
    "        return None, None\n",
    "\n",
    "    # Clean text\n",
    "    print(\"Cleaning text for sentiment model...\")\n",
    "    df['cleaned_text'] = df['reviewText'].apply(clean_text)\n",
    "\n",
    "    # Filter out rows with empty cleaned text\n",
    "    df_filtered = df[df['cleaned_text'].str.len() > 0].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "    if len(df_filtered) == 0:\n",
    "        print(\"Error: No valid text data left after cleaning.\")\n",
    "        return None, None\n",
    "    print(f\"Using {len(df_filtered)} reviews for training after cleaning.\")\n",
    "\n",
    "    # Create features\n",
    "    vectorizer = TfidfVectorizer(max_features=5000)\n",
    "    print(\"Vectorizing text (TF-IDF)...\")\n",
    "    X = vectorizer.fit_transform(df_filtered['cleaned_text'])\n",
    "\n",
    "    # Create target (convert ratings to sentiment classes)\n",
    "    df_filtered['sentiment_class'] = df_filtered['rating'].apply(lambda x: 'positive' if x >= 4 else ('neutral' if x == 3 else 'negative'))\n",
    "    y = df_filtered['sentiment_class']\n",
    "\n",
    "    if len(df_filtered) < 2 or len(y.unique()) < 2:\n",
    "        print(\"Error: Not enough data or classes to split and train.\")\n",
    "        return vectorizer, None # Return vectorizer even if model fails\n",
    "\n",
    "    # Split data\n",
    "    print(\"Splitting data (80% train, 20% test)...\")\n",
    "    try:\n",
    "      X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Added stratify\n",
    "    except ValueError as e:\n",
    "        print(f\"Error during train/test split (likely too few samples in a class): {e}\")\n",
    "        print(\"Skipping model training.\")\n",
    "        return vectorizer, None\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    print(\"Training Naive Bayes model...\")\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"Evaluating model...\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Sentiment model accuracy: {accuracy:.4f}\")\n",
    "    print(classification_report(y_test, y_pred, zero_division=0))\n",
    "\n",
    "    # Create confusion matrix\n",
    "    try:\n",
    "        cm = confusion_matrix(y_test, y_pred, labels=model.classes_)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                    xticklabels=model.classes_,\n",
    "                    yticklabels=model.classes_)\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('True')\n",
    "        plt.title('Sentiment Confusion Matrix')\n",
    "        plt.savefig('./results/sentiment_confusion_matrix.png')\n",
    "        plt.close()\n",
    "        print(\"Confusion matrix saved to ./results/sentiment_confusion_matrix.png\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate confusion matrix plot: {e}\")\n",
    "\n",
    "    print(\"--- Sentiment Model Training Finished ---\")\n",
    "    return vectorizer, model\n",
    "\n",
    "# --- Fake Review Detection (Placeholder) ---\n",
    "def detect_fake_reviews(df):\n",
    "    \"\"\"Detect potentially fake reviews (Placeholder).\"\"\"\n",
    "    print(\"\\n--- Detecting Fake Reviews (Placeholder Logic) ---\")\n",
    "    if 'rating' not in df.columns:\n",
    "         print(\"Warning: 'rating' column missing for fake review analysis. Skipping.\")\n",
    "         df['is_fake'] = False\n",
    "         df['suspicious_score'] = 0.0\n",
    "         return df\n",
    "\n",
    "    # Using placeholder logic: Assume reviews with extreme ratings (1 or 5) and short text might be slightly more suspicious\n",
    "    min_text_len_for_genuine = 15 # Arbitrary threshold for text length\n",
    "    print(f\"Flagging reviews with rating 1 or 5 AND text length < {min_text_len_for_genuine} chars.\")\n",
    "    # Ensure reviewText is string and handle NaN\n",
    "    df['reviewText_str'] = df['reviewText'].astype(str).fillna('')\n",
    "    is_fake_condition = ((df['rating'] == 1) | (df['rating'] == 5)) & (df['reviewText_str'].str.len() < min_text_len_for_genuine)\n",
    "    is_fake = np.where(is_fake_condition, True, False)\n",
    "\n",
    "    # Add results to DataFrame\n",
    "    df['is_fake'] = is_fake\n",
    "    # Placeholder score - slightly higher for those flagged by the simple rule\n",
    "    df['suspicious_score'] = np.random.uniform(0.1, 0.3, size=len(df)) # Base random score\n",
    "    df.loc[df['is_fake'], 'suspicious_score'] = np.random.uniform(0.5, 0.7, size=df['is_fake'].sum()) # Higher score for flagged\n",
    "    print(f\"Flagged {df['is_fake'].sum()} reviews based on placeholder logic.\")\n",
    "\n",
    "    # Create visualizations\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(x='rating', hue='is_fake', data=df, palette={True: \"red\", False: \"green\"})\n",
    "        plt.title('Rating Distribution: Flagged vs. Not Flagged Reviews (Placeholder)')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig('./results/fake_rating_distribution.png')\n",
    "        plt.close()\n",
    "        print(\"Rating distribution plot saved to ./results/fake_rating_distribution.png\")\n",
    "\n",
    "        # Fake review percentage by category\n",
    "        if 'category' in df.columns:\n",
    "            fake_by_category = df.groupby('category')['is_fake'].mean().reset_index()\n",
    "            fake_by_category = fake_by_category.sort_values('is_fake', ascending=False)\n",
    "\n",
    "            if not fake_by_category.empty:\n",
    "                plt.figure(figsize=(10, max(6, len(fake_by_category) * 0.5))) # Adjust height\n",
    "                sns.barplot(x='is_fake', y='category', data=fake_by_category, palette=\"viridis\")\n",
    "                plt.title('Flagged Review Percentage by Category (Placeholder)')\n",
    "                plt.xlabel('Percentage of Flagged Reviews')\n",
    "                plt.ylabel('Category')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('./results/fake_category_distribution.png')\n",
    "                plt.close()\n",
    "                print(\"Category distribution plot saved to ./results/fake_category_distribution.png\")\n",
    "            else:\n",
    "                 print(\"No category data to generate category distribution plot.\")\n",
    "        else:\n",
    "            print(\"Skipping category distribution plot: 'category' column missing.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate fake review plots: {e}\")\n",
    "    print(\"--- Fake Review Detection Finished ---\")\n",
    "    return df\n",
    "\n",
    "# --- Aspect Analysis (Placeholder) ---\n",
    "def analyze_aspects(df):\n",
    "    \"\"\"Analyze aspects mentioned in reviews (Placeholder).\"\"\"\n",
    "    print(\"\\n--- Analyzing Review Aspects (Placeholder: Nouns) ---\")\n",
    "    if 'reviewText' not in df.columns:\n",
    "        print(\"Warning: 'reviewText' column missing for aspect analysis. Skipping.\")\n",
    "        df['detected_aspects'] = [[] for _ in range(len(df))]\n",
    "        return df\n",
    "\n",
    "    # Detect aspects for each review (Placeholder: extract nouns)\n",
    "    print(\"Applying placeholder aspect extraction (extracting nouns)...\")\n",
    "    aspects_list = []\n",
    "\n",
    "    # Batch processing for potentially faster POS tagging\n",
    "    # Clean text first\n",
    "    cleaned_texts = df['reviewText'].apply(clean_text)\n",
    "    # Tokenize and tag\n",
    "    try:\n",
    "        tokens_list = [nltk.word_tokenize(text) for text in cleaned_texts]\n",
    "        tagged_list = nltk.pos_tag_sents(tokens_list) # Use sentence-based tagging if reviews are long\n",
    "        # Extract nouns (NN, NNS, NNP, NNPS)\n",
    "        for tagged_sent in tagged_list:\n",
    "            detected_aspects = [word for word, tag in tagged_sent if tag.startswith('NN')]\n",
    "            aspects_list.append(list(set(detected_aspects[:10]))) # Keep unique, max 10 per review\n",
    "    except Exception as e:\n",
    "        print(f\"Error during batch POS tagging: {e}. Falling back to row-by-row (slower).\")\n",
    "        aspects_list = [] # Reset list\n",
    "        for text in cleaned_texts:\n",
    "            try:\n",
    "                tokens = nltk.word_tokenize(text)\n",
    "                tagged = nltk.pos_tag(tokens)\n",
    "                detected_aspects = [word for word, tag in tagged if tag.startswith('NN')]\n",
    "                aspects_list.append(list(set(detected_aspects[:10])))\n",
    "            except Exception:\n",
    "                aspects_list.append([]) # Handle potential errors\n",
    "\n",
    "    df['detected_aspects'] = aspects_list\n",
    "    print(f\"Processed {len(aspects_list)} reviews for aspects.\")\n",
    "\n",
    "    # Count aspect occurrences\n",
    "    all_aspects = [aspect for sublist in aspects_list for aspect in sublist]\n",
    "\n",
    "    if not all_aspects:\n",
    "        print(\"No aspects found.\")\n",
    "        print(\"--- Aspect Analysis Finished ---\")\n",
    "        return df\n",
    "\n",
    "    aspect_counts = pd.Series(all_aspects).value_counts().reset_index()\n",
    "    aspect_counts.columns = ['aspect', 'count']\n",
    "    print(f\"Found {len(aspect_counts)} unique aspects.\")\n",
    "\n",
    "    # Create visualization\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        top_n = min(20, len(aspect_counts)) # Show top 20\n",
    "        sns.barplot(x='count', y='aspect', data=aspect_counts.head(top_n), palette=\"magma\")\n",
    "        plt.title(f'Top {top_n} Most Common Potential Aspects (Placeholder Nouns)')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Aspect')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./results/aspect_distribution.png')\n",
    "        plt.close()\n",
    "        print(f\"Aspect distribution plot saved to ./results/aspect_distribution.png (Top {top_n})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate aspect distribution plot: {e}\")\n",
    "\n",
    "    print(\"--- Aspect Analysis Finished ---\")\n",
    "    return df\n",
    "\n",
    "# --- Emotion Analysis (Simplified) ---\n",
    "def analyze_emotions(df):\n",
    "    \"\"\"Analyze emotions expressed in reviews (Simplified based on rating).\"\"\"\n",
    "    print(\"\\n--- Analyzing Emotions (Based on Rating) ---\")\n",
    "    if 'rating' not in df.columns:\n",
    "        print(\"Warning: 'rating' column missing for emotion analysis. Skipping.\")\n",
    "        df['emotion'] = 'unknown'\n",
    "        return df\n",
    "\n",
    "    # Map ratings to emotions\n",
    "    emotion_map = {\n",
    "        1: 'angry',\n",
    "        2: 'disappointed',\n",
    "        3: 'neutral',\n",
    "        4: 'satisfied',\n",
    "        5: 'delighted'\n",
    "    }\n",
    "    df['emotion'] = df['rating'].map(emotion_map).fillna('unknown') # Handle potential NaN ratings\n",
    "    print(\"Mapped ratings to emotions.\")\n",
    "\n",
    "    # Create visualization\n",
    "    try:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        emotion_order = ['delighted', 'satisfied', 'neutral', 'disappointed', 'angry', 'unknown']\n",
    "        # Filter order to only include emotions present in the data\n",
    "        actual_emotions = df['emotion'].unique()\n",
    "        ordered_actual_emotions = [e for e in emotion_order if e in actual_emotions]\n",
    "\n",
    "        sns.countplot(y='emotion', data=df, order=ordered_actual_emotions, palette=\"coolwarm\")\n",
    "        plt.title('Distribution of Emotions in Reviews (Based on Rating)')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Emotion')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('./results/emotion_distribution.png')\n",
    "        plt.close()\n",
    "        print(\"Emotion distribution plot saved to ./results/emotion_distribution.png\")\n",
    "\n",
    "        # Emotion trends over time (Check for datetime column)\n",
    "        if 'datetime' in df.columns and pd.api.types.is_datetime64_any_dtype(df['datetime']):\n",
    "            print(\"Analyzing emotion trends over time...\")\n",
    "            df_trends = df.dropna(subset=['datetime']).copy() # Drop rows with NaT datetimes\n",
    "            df_trends['date'] = df_trends['datetime'].dt.date\n",
    "            # Resample to monthly if reasonable number of dates\n",
    "            resample_period = 'M' # 'W' for weekly, 'M' for monthly\n",
    "            try:\n",
    "                emotion_trends = df_trends.set_index('datetime').groupby('emotion').resample(resample_period).size().unstack(level=0).fillna(0)\n",
    "\n",
    "                if not emotion_trends.empty and len(emotion_trends) > 1: # Need at least 2 periods for a trend\n",
    "                    plt.figure(figsize=(12, 8))\n",
    "                    emotion_trends.plot(kind='line', marker='.', linestyle='-') # Adjusted style\n",
    "                    plt.title(f'Emotion Trends Over Time ({resample_period}-based)')\n",
    "                    plt.xlabel('Date')\n",
    "                    plt.ylabel('Review Count')\n",
    "                    plt.legend(title='Emotion', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                    plt.grid(True, axis='y', linestyle='--')\n",
    "                    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for legend\n",
    "                    plt.savefig('./results/emotion_trends.png')\n",
    "                    plt.close()\n",
    "                    print(\"Emotion trends plot saved to ./results/emotion_trends.png\")\n",
    "                else:\n",
    "                    print(\"Not enough data points or time periods for meaningful trend analysis after resampling.\")\n",
    "            except Exception as e:\n",
    "                 print(f\"Could not generate emotion trends plot: {e}\")\n",
    "        else:\n",
    "            print(\"Skipping emotion trend analysis: 'datetime' column not found, not datetime type, or empty.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate emotion distribution plot: {e}\")\n",
    "\n",
    "    print(\"--- Emotion Analysis Finished ---\")\n",
    "    return df\n",
    "\n",
    "# --- Model Saving ---\n",
    "def save_model(vectorizer, model, identifier=\"sentiment\"):\n",
    "    \"\"\"Save the trained model and vectorizer.\"\"\"\n",
    "    if vectorizer is None or model is None:\n",
    "        print(\"Skipping model saving: vectorizer or model is None.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        model_filename = f'./results/model_{identifier}.pkl'\n",
    "        vectorizer_filename = f'./results/vectorizer_{identifier}.pkl'\n",
    "        with open(model_filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        with open(vectorizer_filename, 'wb') as f:\n",
    "            pickle.dump(vectorizer, f)\n",
    "        print(f\"Model saved to {model_filename}\")\n",
    "        print(f\"Vectorizer saved to {vectorizer_filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model/vectorizer: {e}\")\n",
    "\n",
    "\n",
    "# --- Processing Workflows ---\n",
    "def batch_process(df_input):\n",
    "    \"\"\"Process the entire dataset in batch mode.\"\"\"\n",
    "    print(\"\\n★★★ Starting Batch Processing Workflow ★★★\")\n",
    "    start_time = time.time()\n",
    "    if df_input is None or df_input.empty:\n",
    "        print(\"Error: Input DataFrame is empty. Cannot perform batch processing.\")\n",
    "        return None\n",
    "    df_processed = df_input.copy() # Work on a copy\n",
    "\n",
    "    # --- Steps ---\n",
    "    # 1. Train sentiment model\n",
    "    vectorizer, model = train_sentiment_model(df_processed)\n",
    "    if model is not None:\n",
    "        save_model(vectorizer, model, identifier=\"batch_sentiment\")\n",
    "\n",
    "    # 2. Detect fake reviews\n",
    "    df_processed = detect_fake_reviews(df_processed)\n",
    "\n",
    "    # 3. Analyze aspects\n",
    "    df_processed = analyze_aspects(df_processed)\n",
    "\n",
    "    # 4. Analyze emotions\n",
    "    df_processed = analyze_emotions(df_processed)\n",
    "    # --- End Steps ---\n",
    "\n",
    "    # Save processed data\n",
    "    try:\n",
    "        output_file = \"./results/processed_amazon_reviews_batch.parquet\"\n",
    "        # Select relevant columns to save if df gets too large\n",
    "        columns_to_save = [col for col in ['reviewText', 'rating', 'category', 'datetime', 'cleaned_text', 'sentiment_class', 'is_fake', 'suspicious_score', 'detected_aspects', 'emotion'] if col in df_processed.columns]\n",
    "        df_processed[columns_to_save].to_parquet(output_file, index=False)\n",
    "        print(f\"\\nBatch processed data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving batch processed data: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"★★★ Batch Processing Completed (Duration: {end_time - start_time:.2f} seconds) ★★★\")\n",
    "    return df_processed\n",
    "\n",
    "def stream_process(df_input, interval=100):\n",
    "    \"\"\"Process the dataset in streaming mode (Simulation).\"\"\"\n",
    "    print(f\"\\n★★★ Starting Stream Processing Simulation (Chunk Size: {interval}) ★★★\")\n",
    "    start_time = time.time()\n",
    "    if df_input is None or df_input.empty:\n",
    "        print(\"Error: Input DataFrame is empty. Cannot perform stream processing simulation.\")\n",
    "        return None\n",
    "    df_processed = df_input.copy() # Work on a copy\n",
    "    df_processed['stream_sentiment_score'] = np.nan # Initialize column for stream results\n",
    "\n",
    "    # Initialize self-adaptive model (Placeholder - if you had one)\n",
    "    # adaptive_model = SelfAdaptiveModel() #Commented out\n",
    "\n",
    "    # Process in chunks to simulate streaming\n",
    "    num_reviews = len(df_processed)\n",
    "    chunk_size = min(interval, num_reviews)\n",
    "    if chunk_size <= 0:\n",
    "        print(\"No data to process in stream mode.\")\n",
    "        return df_processed\n",
    "\n",
    "    num_chunks = (num_reviews + chunk_size - 1) // chunk_size # Handle last partial chunk\n",
    "    print(f\"Total reviews: {num_reviews}, Chunks: {num_chunks}\")\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min(start_idx + chunk_size, num_reviews)\n",
    "        # Use .loc for slicing to ensure we modify the original df_processed copy\n",
    "        chunk_indices = df_processed.index[start_idx:end_idx]\n",
    "        chunk = df_processed.loc[chunk_indices]\n",
    "\n",
    "        print(f\"\\nProcessing chunk {i+1}/{num_chunks} (Indices {start_idx}-{end_idx-1}, {len(chunk)} reviews)...\")\n",
    "\n",
    "        # --- Stream Processing Logic ---\n",
    "        # Placeholder: Assign random sentiment score\n",
    "        # Replace with actual model prediction (e.g., adaptive_model.predict(chunk))\n",
    "        print(\"Applying placeholder stream sentiment analysis...\")\n",
    "        chunk_scores = np.random.uniform(-1, 1, size=len(chunk))\n",
    "        # -----------------------------\n",
    "\n",
    "        # Update the main DataFrame slice directly using .loc\n",
    "        df_processed.loc[chunk_indices, 'stream_sentiment_score'] = chunk_scores\n",
    "        print(f\"Updated sentiment scores for chunk {i+1}.\")\n",
    "\n",
    "        # Simulate real-time processing delay (optional)\n",
    "        # time.sleep(0.05)\n",
    "\n",
    "    # Save processed data for streaming simulation\n",
    "    try:\n",
    "        output_file = \"./results/processed_amazon_reviews_stream.parquet\"\n",
    "        columns_to_save = [col for col in ['reviewText', 'rating', 'category', 'datetime', 'stream_sentiment_score'] if col in df_processed.columns]\n",
    "        df_processed[columns_to_save].to_parquet(output_file, index=False)\n",
    "        print(f\"\\nStream processed data (with scores) saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving stream processed data: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"★★★ Stream Processing Simulation Completed (Duration: {end_time - start_time:.2f} seconds) ★★★\")\n",
    "    return df_processed\n",
    "\n",
    "# --- Main Function (primarily for Command-Line Execution) ---\n",
    "# This function uses argparse and is intended for running the code as a script.\n",
    "# It WILL NOT be executed automatically when this cell is run in Jupyter.\n",
    "def main():\n",
    "    \"\"\"Main function to orchestrate the analysis pipeline via command line.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Amazon Review Analysis System (Script Mode)')\n",
    "    parser.add_argument('--categories', type=str, nargs='+',\n",
    "                        default=['Electronics', 'Books', 'Home', 'Health'], # Default categories for placeholder\n",
    "                        help='Categories to analyze (used if loading from HF)')\n",
    "    parser.add_argument('--mode', type=str, choices=['batch', 'stream'], default='batch',\n",
    "                        help='Processing mode: batch or stream')\n",
    "    parser.add_argument('--sample_size', type=int, default=1000, # Smaller default for script testing\n",
    "                        help='Number of reviews to sample (used if loading/generating placeholder)')\n",
    "    parser.add_argument('--input_file', type=str, default=\"filtered_amazon_reviews.parquet\",\n",
    "                        help='Path to pre-filtered Parquet file')\n",
    "    parser.add_argument('--use_hf', action='store_true',\n",
    "                        help='Force loading from Hugging Face (placeholder generation)')\n",
    "    parser.add_argument('--stream_interval', type=int, default=100,\n",
    "                        help='Chunk size for stream processing simulation')\n",
    "\n",
    "    # This line WILL CAUSE AN ERROR if main() is called directly within Jupyter\n",
    "    # because Jupyter passes its own arguments like -f\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    print(f\"\\n--- Running Analysis via main() [Script Mode] ---\")\n",
    "    print(f\"Mode: {args.mode}, Categories: {args.categories}, Sample Size: {args.sample_size}\")\n",
    "    print(f\"Input File: {args.input_file}, Use Placeholder/HF: {args.use_hf}, Stream Interval: {args.stream_interval}\")\n",
    "\n",
    "    # --- Data Loading (Script Mode) ---\n",
    "    df_script = None\n",
    "    if not args.use_hf:\n",
    "        try:\n",
    "            print(f\"Attempting to load script data from: {args.input_file}\")\n",
    "            df_script = pd.read_parquet(args.input_file)\n",
    "            print(f\"Loaded {len(df_script)} reviews from '{args.input_file}'.\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File '{args.input_file}' not found.\")\n",
    "            if not args.use_hf:\n",
    "                print(\"Falling back to generating placeholder data.\")\n",
    "                args.use_hf = True\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file '{args.input_file}': {e}\")\n",
    "            print(\"Falling back to generating placeholder data.\")\n",
    "            args.use_hf = True\n",
    "\n",
    "    if args.use_hf or df_script is None:\n",
    "        print(f\"Generating placeholder data for script execution (Size: {args.sample_size})...\")\n",
    "        n_samples = args.sample_size\n",
    "        texts = [f\"Script review {i+1} about a {np.random.choice(args.categories)} item.\" for i in range(n_samples)]\n",
    "        ratings = np.random.randint(1, 6, size=n_samples)\n",
    "        cats = np.random.choice(args.categories, size=n_samples)\n",
    "        df_script = pd.DataFrame({\n",
    "            'reviewText': texts,\n",
    "            'rating': ratings,\n",
    "            'category': cats,\n",
    "            'datetime': pd.to_datetime(np.random.randint(1640995200, 1672531199, size=n_samples), unit='s') # Random dates\n",
    "            })\n",
    "        print(f\"Generated {len(df_script)} placeholder reviews for script.\")\n",
    "        # Optionally save this generated data\n",
    "        # try:\n",
    "        #     df_script.to_parquet(args.input_file)\n",
    "        #     print(f\"Saved generated placeholder data to '{args.input_file}'.\")\n",
    "        # except Exception as e:\n",
    "        #      print(f\"Could not save generated placeholder data: {e}\")\n",
    "\n",
    "    if df_script is None or df_script.empty:\n",
    "        print(\"Error: No data loaded or generated. Exiting script mode.\")\n",
    "        return # Exit if data loading failed\n",
    "\n",
    "    print(f\"Proceeding with {len(df_script)} reviews in script mode.\")\n",
    "\n",
    "    # --- Process data based on mode (Script Mode) ---\n",
    "    if args.mode == 'batch':\n",
    "        df_processed_script = batch_process(df_script)\n",
    "    elif args.mode == 'stream':\n",
    "        df_processed_script = stream_process(df_script, interval=args.stream_interval)\n",
    "    else:\n",
    "        print(f\"Error: Unknown mode '{args.mode}' in script execution.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n--- Analysis via main() [Script Mode] completed successfully. ---\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 3: Interactive Execution Configuration\n",
    "# =============================================================================\n",
    "# Configure parameters for running the analysis interactively within this notebook cell\n",
    "\n",
    "# --- Data Loading Config ---\n",
    "interactive_input_parquet_file = \"filtered_amazon_reviews.parquet\"\n",
    "interactive_use_placeholder_data = False # Set to True to generate placeholder data if file is not found\n",
    "interactive_placeholder_sample_size = 500 # Number of placeholder reviews to generate\n",
    "interactive_placeholder_categories = ['Electronics', 'Books', 'Home', 'Health', 'Toys']\n",
    "\n",
    "# --- Analysis Config ---\n",
    "# Choose the mode: 'batch' or 'stream'\n",
    "interactive_analysis_mode = 'batch'\n",
    "# interactive_analysis_mode = 'stream'\n",
    "\n",
    "# Set stream interval if using stream mode\n",
    "interactive_stream_interval = 50 # Process in smaller chunks for faster feedback\n",
    "\n",
    "# =============================================================================\n",
    "# Section 4: Interactive Data Loading (Runs when cell executes)\n",
    "# =============================================================================\n",
    "print(\"\\n===============================================\")\n",
    "print(\"=== Starting Interactive Data Loading Section ===\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "df_interactive = None # Initialize dataframe variable\n",
    "\n",
    "try:\n",
    "    print(f\"\\nAttempting to load interactive data from: {interactive_input_parquet_file}\")\n",
    "    df_interactive = pd.read_parquet(interactive_input_parquet_file)\n",
    "    print(f\"Successfully loaded {len(df_interactive)} reviews from '{interactive_input_parquet_file}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"File '{interactive_input_parquet_file}' not found.\")\n",
    "    if interactive_use_placeholder_data:\n",
    "        print(f\"\\nGenerating {interactive_placeholder_sample_size} placeholder reviews for interactive use...\")\n",
    "        n_samples = interactive_placeholder_sample_size\n",
    "        categories = interactive_placeholder_categories\n",
    "        texts = [f\"Interactive placeholder review {i+1} about a {categories[i%len(categories)]} item.\" for i in range(n_samples)]\n",
    "        ratings = np.random.randint(1, 6, size=n_samples)\n",
    "        cats = np.random.choice(categories, size=n_samples)\n",
    "        df_interactive = pd.DataFrame({\n",
    "            'reviewText': texts,\n",
    "            'rating': ratings,\n",
    "            'category': cats,\n",
    "            'datetime': pd.to_datetime(np.random.randint(1640995200, 1704067199, size=n_samples), unit='s') # Random dates 2022-2023\n",
    "            })\n",
    "        print(f\"Generated {len(df_interactive)} placeholder reviews.\")\n",
    "        # Optionally save the generated data\n",
    "        # try:\n",
    "        #     df_interactive.to_parquet(interactive_input_parquet_file)\n",
    "        #     print(f\"Saved generated placeholder data to '{interactive_input_parquet_file}'.\")\n",
    "        # except Exception as e:\n",
    "        #     print(f\"Could not save generated placeholder data: {e}\")\n",
    "    else:\n",
    "        print(\"\\nPlaceholder data generation is disabled ('interactive_use_placeholder_data' is False).\")\n",
    "        print(f\"Please create the file '{interactive_input_parquet_file}' or set the flag to True.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred loading '{interactive_input_parquet_file}': {e}\")\n",
    "\n",
    "# Display the first few rows if data was loaded/generated\n",
    "if df_interactive is not None and not df_interactive.empty:\n",
    "    print(\"\\nInteractive Data loaded/generated successfully:\")\n",
    "    display(df_interactive.head())\n",
    "    print(f\"\\nInteractive DataFrame Info:\")\n",
    "    df_interactive.info()\n",
    "else:\n",
    "    print(\"\\nNo interactive data available for processing.\")\n",
    "\n",
    "print(\"\\n=== Finished Interactive Data Loading Section ===\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 5: Interactive Analysis Execution (Runs when cell executes)\n",
    "# =============================================================================\n",
    "print(\"\\n===================================================\")\n",
    "print(\"=== Starting Interactive Analysis Execution Section ===\")\n",
    "print(\"===================================================\")\n",
    "\n",
    "# --- Check if data is loaded ---\n",
    "if df_interactive is not None and not df_interactive.empty:\n",
    "    print(f\"\\nStarting interactive analysis in '{interactive_analysis_mode}' mode...\")\n",
    "    df_processed_interactive = None # Initialize result df\n",
    "\n",
    "    if interactive_analysis_mode == 'batch':\n",
    "        # Run the batch process function\n",
    "        df_processed_interactive = batch_process(df_interactive) # Pass the loaded interactive df\n",
    "\n",
    "    elif interactive_analysis_mode == 'stream':\n",
    "        # Run the stream process function\n",
    "        df_processed_interactive = stream_process(df_interactive, interval=interactive_stream_interval) # Pass the loaded df and interval\n",
    "\n",
    "    else:\n",
    "        print(f\"Error: Unknown interactive mode '{interactive_analysis_mode}'. Choose 'batch' or 'stream' in Section 3.\")\n",
    "\n",
    "    # Display results if processing occurred\n",
    "    if df_processed_interactive is not None:\n",
    "         print(f\"\\n--- Interactive '{interactive_analysis_mode}' Analysis Finished ---\")\n",
    "         print(\"Processed DataFrame head:\")\n",
    "         display(df_processed_interactive.head())\n",
    "         # Optional: display more details\n",
    "         # print(\"\\nColumns added/modified:\")\n",
    "         # display(df_processed_interactive.columns)\n",
    "         # if 'sentiment_class' in df_processed_interactive.columns: display(df_processed_interactive[['rating', 'sentiment_class']].head())\n",
    "         # if 'is_fake' in df_processed_interactive.columns: display(df_processed_interactive[['rating', 'is_fake', 'suspicious_score']].head())\n",
    "         # if 'detected_aspects' in df_processed_interactive.columns: display(df_processed_interactive[['detected_aspects']].head())\n",
    "         # if 'emotion' in df_processed_interactive.columns: display(df_processed_interactive[['rating', 'emotion']].head())\n",
    "         # if 'stream_sentiment_score' in df_processed_interactive.columns: display(df_processed_interactive[['stream_sentiment_score']].head())\n",
    "    else:\n",
    "        print(\"\\nAnalysis function did not return a DataFrame (likely due to an error during processing).\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCannot run analysis: Interactive data ('df_interactive') was not loaded successfully in Section 4.\")\n",
    "\n",
    "print(\"\\n=== Finished Interactive Analysis Execution Section ===\")\n",
    "print(\"===================================================\")\n",
    "print(\"=== End of Cell Execution ===\")\n",
    "print(\"===================================================\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Section 6: Guard for Command-Line Execution\n",
    "# =============================================================================\n",
    "# This block ensures main() is ONLY called when the script is run directly\n",
    "# from the command line (e.g., `python your_script_name.py --mode batch`).\n",
    "# It WILL NOT run automatically when this cell is executed in Jupyter Notebook/Lab.\n",
    "if __name__ == \"__main__\":\n",
    "    # Note: If run as a script, the interactive sections 4 and 5 above will still execute\n",
    "    # before this main() function is called. Consider refactoring further if cleaner\n",
    "    # separation between interactive and script execution is needed for a .py file.\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6dd1b8b-de47-49c2-92cf-14ca3e9a999e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\Manvi Bhala\\anaconda3\\envs\\tf_env\\lib\\site-packages\\transformers\\pipelines\\text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emotion analyzer initialized with model: j-hartmann/emotion-english-distilroberta-base\n",
      "Emotion analysis for 'I absolutely love this product! It exceeded all my expectations.':\n",
      "  anger: 0.0045\n",
      "  disgust: 0.0012\n",
      "  fear: 0.0012\n",
      "  joy: 0.9558\n",
      "  neutral: 0.0092\n",
      "  sadness: 0.0016\n",
      "  surprise: 0.0264\n",
      "Loaded 10000 reviews.\n",
      "Analyzing emotions in 100 reviews...\n",
      "Emotion analysis completed.\n",
      "Emotion visualizations saved to ./results\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x1000 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1400x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "class EmotionAnalyzer:\n",
    "    def __init__(self, model_name=\"j-hartmann/emotion-english-distilroberta-base\"):\n",
    "        \"\"\"\n",
    "        Initialize the EmotionAnalyzer with a pre-trained emotion detection model.\n",
    "        \n",
    "        Args:\n",
    "            model_name (str): The name of the pre-trained model to use.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.classifier = pipeline(\"text-classification\", \n",
    "                                      model=model_name, \n",
    "                                      return_all_scores=True)\n",
    "            self.emotions = [\"joy\", \"sadness\", \"anger\", \"fear\", \"surprise\", \"disgust\", \"neutral\"]\n",
    "            print(f\"Emotion analyzer initialized with model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing emotion analyzer: {e}\")\n",
    "            self.classifier = None\n",
    "    \n",
    "    def analyze_text(self, text):\n",
    "        \"\"\"\n",
    "        Analyze the emotions in a text.\n",
    "        \n",
    "        Args:\n",
    "            text (str): The text to analyze.\n",
    "            \n",
    "        Returns:\n",
    "            dict: A dictionary mapping emotions to their scores.\n",
    "        \"\"\"\n",
    "        if not self.classifier or not isinstance(text, str) or len(text.strip()) == 0:\n",
    "            return {emotion: 0.0 for emotion in self.emotions}\n",
    "        \n",
    "        try:\n",
    "            # Truncate text if it's too long\n",
    "            max_length = 512\n",
    "            if len(text) > max_length:\n",
    "                text = text[:max_length]\n",
    "            \n",
    "            # Get emotion scores\n",
    "            result = self.classifier(text)[0]\n",
    "            \n",
    "            # Convert to dictionary\n",
    "            emotion_scores = {item['label']: item['score'] for item in result}\n",
    "            \n",
    "            return emotion_scores\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing text: {e}\")\n",
    "            return {emotion: 0.0 for emotion in self.emotions}\n",
    "    \n",
    "    def analyze_batch(self, texts, batch_size=32):\n",
    "        \"\"\"\n",
    "        Analyze emotions in a batch of texts.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): A list of texts to analyze.\n",
    "            batch_size (int): The batch size for processing.\n",
    "            \n",
    "        Returns:\n",
    "            list: A list of dictionaries mapping emotions to their scores.\n",
    "        \"\"\"\n",
    "        if not self.classifier:\n",
    "            return [{emotion: 0.0 for emotion in self.emotions} for _ in texts]\n",
    "        \n",
    "        results = []\n",
    "        for i in range(0, len(texts), batch_size):\n",
    "            batch = texts[i:i+batch_size]\n",
    "            # Filter out non-string or empty texts\n",
    "            batch = [text for text in batch if isinstance(text, str) and len(text.strip()) > 0]\n",
    "            \n",
    "            if not batch:\n",
    "                continue\n",
    "                \n",
    "            try:\n",
    "                # Truncate texts if they're too long\n",
    "                batch = [text[:512] if len(text) > 512 else text for text in batch]\n",
    "                \n",
    "                # Get emotion scores for batch\n",
    "                batch_results = self.classifier(batch)\n",
    "                \n",
    "                # Convert to dictionaries\n",
    "                for result in batch_results:\n",
    "                    emotion_scores = {item['label']: item['score'] for item in result}\n",
    "                    results.append(emotion_scores)\n",
    "            except Exception as e:\n",
    "                print(f\"Error analyzing batch: {e}\")\n",
    "                results.extend([{emotion: 0.0 for emotion in self.emotions} for _ in batch])\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def analyze_reviews(self, reviews_df, text_column='reviewText'):\n",
    "        \"\"\"\n",
    "        Analyze emotions in a DataFrame of reviews.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews.\n",
    "            text_column (str): The name of the column containing review text.\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: The input DataFrame with additional emotion columns.\n",
    "        \"\"\"\n",
    "        if text_column not in reviews_df.columns:\n",
    "            print(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "            return reviews_df\n",
    "        \n",
    "        # Get texts to analyze\n",
    "        texts = reviews_df[text_column].tolist()\n",
    "        \n",
    "        # Analyze emotions\n",
    "        print(f\"Analyzing emotions in {len(texts)} reviews...\")\n",
    "        emotion_results = self.analyze_batch(texts)\n",
    "        \n",
    "        # Add emotion scores to DataFrame\n",
    "        result_df = reviews_df.copy()\n",
    "        \n",
    "        # Initialize emotion columns\n",
    "        for emotion in self.emotions:\n",
    "            result_df[f'emotion_{emotion}'] = 0.0\n",
    "        \n",
    "        # Fill in emotion scores\n",
    "        for i, emotion_dict in enumerate(emotion_results):\n",
    "            for emotion, score in emotion_dict.items():\n",
    "                result_df.loc[i, f'emotion_{emotion}'] = score\n",
    "        \n",
    "        # Add dominant emotion column\n",
    "        if emotion_results:\n",
    "            result_df['dominant_emotion'] = result_df[[f'emotion_{e}' for e in self.emotions]].idxmax(axis=1)\n",
    "            result_df['dominant_emotion'] = result_df['dominant_emotion'].str.replace('emotion_', '')\n",
    "        \n",
    "        print(\"Emotion analysis completed.\")\n",
    "        return result_df\n",
    "    \n",
    "    def visualize_emotions(self, reviews_df, output_dir='./results'):\n",
    "        \"\"\"\n",
    "        Create visualizations of emotion analysis results.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews with emotion scores.\n",
    "            output_dir (str): Directory to save visualizations.\n",
    "        \"\"\"\n",
    "        if 'dominant_emotion' not in reviews_df.columns:\n",
    "            print(\"No emotion analysis results found in DataFrame.\")\n",
    "            return\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Emotion distribution\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.countplot(y='dominant_emotion', data=reviews_df, \n",
    "                     order=reviews_df['dominant_emotion'].value_counts().index)\n",
    "        plt.title('Distribution of Emotions in Reviews')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Emotion')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(output_dir, 'emotion_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Emotion by category\n",
    "        if 'category' in reviews_df.columns:\n",
    "            plt.figure(figsize=(14, 10))\n",
    "            emotion_by_category = pd.crosstab(reviews_df['category'], reviews_df['dominant_emotion'])\n",
    "            emotion_by_category_pct = emotion_by_category.div(emotion_by_category.sum(axis=1), axis=0)\n",
    "            \n",
    "            emotion_by_category_pct.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "            plt.title('Emotion Distribution by Category')\n",
    "            plt.xlabel('Category')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Emotion')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'emotion_by_category.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        # Emotion by rating\n",
    "        if 'rating' in reviews_df.columns:\n",
    "            plt.figure(figsize=(14, 8))\n",
    "            emotion_by_rating = pd.crosstab(reviews_df['rating'], reviews_df['dominant_emotion'])\n",
    "            emotion_by_rating_pct = emotion_by_rating.div(emotion_by_rating.sum(axis=1), axis=0)\n",
    "            \n",
    "            emotion_by_rating_pct.plot(kind='bar', stacked=True, colormap='viridis')\n",
    "            plt.title('Emotion Distribution by Rating')\n",
    "            plt.xlabel('Rating')\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.legend(title='Emotion')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(output_dir, 'emotion_by_rating.png'))\n",
    "            plt.close()\n",
    "        \n",
    "        print(f\"Emotion visualizations saved to {output_dir}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer\n",
    "    analyzer = EmotionAnalyzer()\n",
    "    \n",
    "    # Example text\n",
    "    text = \"I absolutely love this product! It exceeded all my expectations.\"\n",
    "    result = analyzer.analyze_text(text)\n",
    "    print(f\"Emotion analysis for '{text}':\")\n",
    "    for emotion, score in result.items():\n",
    "        print(f\"  {emotion}: {score:.4f}\")\n",
    "    \n",
    "    # Load reviews\n",
    "    try:\n",
    "        reviews_df = pd.read_parquet(\"filtered_amazon_reviews.parquet\")\n",
    "        print(f\"Loaded {len(reviews_df)} reviews.\")\n",
    "        \n",
    "        # Analyze a sample of reviews\n",
    "        sample_df = reviews_df.sample(min(100, len(reviews_df)))\n",
    "        result_df = analyzer.analyze_reviews(sample_df)\n",
    "        \n",
    "        # Visualize results\n",
    "        analyzer.visualize_emotions(result_df)\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Reviews file not found. Run data_loader.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "110358fc-73da-459a-92e7-eace7cc2d827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10000 reviews.\n",
      "Index(['rating', 'review_title', 'text', 'images', 'asin', 'parent_asin',\n",
      "       'user_id', 'timestamp', 'helpful_vote', 'verified_purchase', 'category',\n",
      "       'datetime', 'reviewText', 'main_category', 'product_title',\n",
      "       'average_rating', 'store'],\n",
      "      dtype='object')\n",
      "Graph too large (9976 nodes), sampling for visualization.\n",
      "Network visualization saved to ./results\\review_network.png\n",
      "\n",
      "Network Metrics:\n",
      "  num_nodes: 9976\n",
      "  num_edges: 4479\n",
      "  avg_degree: 0.8979550922213312\n",
      "  density: 9.002056062369235e-05\n",
      "  num_components: 7992\n",
      "  num_suspicious_clusters: 383\n",
      "\n",
      "Found 383 suspicious clusters.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class ReviewNetworkAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the ReviewNetworkAnalyzer.\n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    def build_reviewer_network(self, reviews_df):\n",
    "        print(reviews_df.columns)\n",
    "        \"\"\"\n",
    "        Build a network of reviewers based on product co-reviewing.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews with reviewer_id and product_id.\n",
    "            \n",
    "        Returns:\n",
    "            nx.Graph: A NetworkX graph representing the reviewer network.\n",
    "        \"\"\"\n",
    "        if 'user_id' not in reviews_df.columns or 'asin' not in reviews_df.columns:\n",
    "            print(\"Required columns 'user_id' and 'asin' not found in DataFrame.\")\n",
    "            return nx.Graph()\n",
    "        \n",
    "        # Create a graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add reviewers as nodes\n",
    "        reviewers = reviews_df['user_id'].unique()\n",
    "        G.add_nodes_from(reviewers)\n",
    "        \n",
    "        # Group reviews by product\n",
    "        product_reviewers = reviews_df.groupby('asin')['user_id'].apply(list)\n",
    "        \n",
    "        # Add edges between reviewers who reviewed the same product\n",
    "        for reviewers_list in product_reviewers:\n",
    "            if len(reviewers_list) > 1:\n",
    "                for i in range(len(reviewers_list)):\n",
    "                    for j in range(i+1, len(reviewers_list)):\n",
    "                        reviewer1 = reviewers_list[i]\n",
    "                        reviewer2 = reviewers_list[j]\n",
    "                        if G.has_edge(reviewer1, reviewer2):\n",
    "                            G[reviewer1][reviewer2]['weight'] += 1\n",
    "                        else:\n",
    "                            G.add_edge(reviewer1, reviewer2, weight=1)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def build_product_network(self, reviews_df):\n",
    "        \"\"\"\n",
    "        Build a network of products based on reviewer overlap.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews with reviewer_id and product_id.\n",
    "            \n",
    "        Returns:\n",
    "            nx.Graph: A NetworkX graph representing the product network.\n",
    "        \"\"\"\n",
    "        if 'user_id' not in reviews_df.columns or 'asin' not in reviews_df.columns:\n",
    "            print(\"Required columns 'user_id' and 'asin' not found in DataFrame.\")\n",
    "            return nx.Graph()\n",
    "        \n",
    "        # Create a graph\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Add products as nodes\n",
    "        products = reviews_df['asin'].unique()\n",
    "        G.add_nodes_from(products)\n",
    "        \n",
    "        # Group reviews by reviewer\n",
    "        reviewer_products = reviews_df.groupby('user_id')['asin'].apply(list)\n",
    "        \n",
    "        # Add edges between products reviewed by the same reviewer\n",
    "        for products_list in reviewer_products:\n",
    "            if len(products_list) > 1:\n",
    "                for i in range(len(products_list)):\n",
    "                    for j in range(i+1, len(products_list)):\n",
    "                        product1 = products_list[i]\n",
    "                        product2 = products_list[j]\n",
    "                        if G.has_edge(product1, product2):\n",
    "                            G[product1][product2]['weight'] += 1\n",
    "                        else:\n",
    "                            G.add_edge(product1, product2, weight=1)\n",
    "        \n",
    "        return G\n",
    "    \n",
    "    def identify_suspicious_clusters(self, G, threshold=0.8):\n",
    "        \"\"\"\n",
    "        Identify suspicious clusters in the network.\n",
    "        \n",
    "        Args:\n",
    "            G (nx.Graph): A NetworkX graph.\n",
    "            threshold (float): Threshold for considering a cluster suspicious.\n",
    "            \n",
    "        Returns:\n",
    "            list: List of suspicious clusters (lists of node IDs).\n",
    "        \"\"\"\n",
    "        # Find connected components\n",
    "        components = list(nx.connected_components(G))\n",
    "        \n",
    "        # Calculate density for each component\n",
    "        suspicious_clusters = []\n",
    "        for component in components:\n",
    "            if len(component) < 3:\n",
    "                continue\n",
    "            \n",
    "            subgraph = G.subgraph(component)\n",
    "            density = nx.density(subgraph)\n",
    "            \n",
    "            if density > threshold:\n",
    "                suspicious_clusters.append(list(component))\n",
    "        \n",
    "        return suspicious_clusters\n",
    "    \n",
    "    def visualize_network(self, G, suspicious_clusters=None, output_dir='./results', filename='review_network.png'):\n",
    "        \"\"\"\n",
    "        Visualize the network with suspicious clusters highlighted.\n",
    "        \n",
    "        Args:\n",
    "            G (nx.Graph): A NetworkX graph.\n",
    "            suspicious_clusters (list): List of suspicious clusters.\n",
    "            output_dir (str): Directory to save the visualization.\n",
    "            filename (str): Filename for the visualization.\n",
    "        \"\"\"\n",
    "        if len(G.nodes()) == 0:\n",
    "            print(\"Empty graph, nothing to visualize.\")\n",
    "            return\n",
    "        \n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Limit visualization to a manageable size\n",
    "        if len(G.nodes()) > 1000:\n",
    "            print(f\"Graph too large ({len(G.nodes())} nodes), sampling for visualization.\")\n",
    "            # Sample nodes\n",
    "            sampled_nodes = list(G.nodes())[:1000]\n",
    "            G = G.subgraph(sampled_nodes)\n",
    "        \n",
    "        plt.figure(figsize=(12, 12))\n",
    "        \n",
    "        # Set node colors\n",
    "        node_colors = ['#1f77b4'] * len(G.nodes())  # Default blue\n",
    "        \n",
    "        # Highlight suspicious clusters\n",
    "        if suspicious_clusters:\n",
    "            for i, cluster in enumerate(suspicious_clusters):\n",
    "                for node in cluster:\n",
    "                    if node in G.nodes():\n",
    "                        node_idx = list(G.nodes()).index(node)\n",
    "                        node_colors[node_idx] = '#d62728'  # Red for suspicious\n",
    "        \n",
    "        # Set node sizes based on degree\n",
    "        node_sizes = [10 + 5 * G.degree(node) for node in G.nodes()]\n",
    "        \n",
    "        # Set edge widths based on weight\n",
    "        edge_widths = [0.5 + 0.5 * G[u][v].get('weight', 1) for u, v in G.edges()]\n",
    "        \n",
    "        # Draw the network\n",
    "        pos = nx.spring_layout(G, seed=42)\n",
    "        nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color=node_colors, alpha=0.7)\n",
    "        nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.3, edge_color='gray')\n",
    "        \n",
    "        plt.title('Review Network Analysis')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save the figure\n",
    "        plt.savefig(os.path.join(output_dir, filename), dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        print(f\"Network visualization saved to {os.path.join(output_dir, filename)}\")\n",
    "    \n",
    "    def analyze_review_network(self, reviews_df, output_dir='./results'):\n",
    "        \"\"\"\n",
    "        Perform a complete network analysis on the reviews.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews.\n",
    "            output_dir (str): Directory to save results.\n",
    "            \n",
    "        Returns:\n",
    "            dict: Dictionary containing analysis results.\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        # Build reviewer network\n",
    "        reviewer_network = self.build_reviewer_network(reviews_df)\n",
    "        results['reviewer_network'] = reviewer_network\n",
    "        \n",
    "        # Identify suspicious clusters\n",
    "        suspicious_clusters = self.identify_suspicious_clusters(reviewer_network)\n",
    "        results['suspicious_clusters'] = suspicious_clusters\n",
    "        \n",
    "        # Visualize network\n",
    "        self.visualize_network(reviewer_network, suspicious_clusters, output_dir)\n",
    "        \n",
    "        # Calculate network metrics\n",
    "        if len(reviewer_network.nodes()) > 0:\n",
    "            results['network_metrics'] = {\n",
    "                'num_nodes': len(reviewer_network.nodes()),\n",
    "                'num_edges': len(reviewer_network.edges()),\n",
    "                'avg_degree': sum(dict(reviewer_network.degree()).values()) / len(reviewer_network.nodes()),\n",
    "                'density': nx.density(reviewer_network),\n",
    "                'num_components': nx.number_connected_components(reviewer_network),\n",
    "                'num_suspicious_clusters': len(suspicious_clusters)\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize analyzer\n",
    "    analyzer = ReviewNetworkAnalyzer()\n",
    "    \n",
    "    # Load reviews\n",
    "    try:\n",
    "        reviews_df = pd.read_parquet(\"filtered_amazon_reviews.parquet\")\n",
    "        print(f\"Loaded {len(reviews_df)} reviews.\")\n",
    "        \n",
    "        # Analyze network\n",
    "        results = analyzer.analyze_review_network(reviews_df)\n",
    "        \n",
    "        # Print results\n",
    "        if 'network_metrics' in results:\n",
    "            print(\"\\nNetwork Metrics:\")\n",
    "            for metric, value in results['network_metrics'].items():\n",
    "                print(f\"  {metric}: {value}\")\n",
    "        \n",
    "        if 'suspicious_clusters' in results:\n",
    "            print(f\"\\nFound {len(results['suspicious_clusters'])} suspicious clusters.\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(\"Reviews file not found. Run data_loader.py first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e524d-6fa6-48d9-8857-ee83350a0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking/downloading NLTK resources (punkt, stopwords)...\n",
      "NLTK resources checked.\n",
      "\n",
      "--- Running Aspect Analysis Example ---\n",
      "Attempting to load reviews from: filtered_amazon_reviews.parquet\n",
      "Successfully loaded 10000 reviews.\n",
      "AspectDetector instance created successfully within AspectAnalyzer.\n",
      "\n",
      "--- Starting Aspect Extraction (using 'reviewText' and 'category') ---\n",
      "Processing 10000 reviews...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import time # For timing aspect extraction\n",
    "\n",
    "# ==============================================================\n",
    "# IMPORTANT ASSUMPTION:\n",
    "# The 'AspectDetector' class MUST be defined and executed\n",
    "# in a previous cell or part of the script BEFORE this code runs.\n",
    "# We are removing the `from gnn_context_model import AspectDetector` line\n",
    "# because AspectDetector is assumed to be already available in the scope.\n",
    "# ==============================================================\n",
    "\n",
    "# Download NLTK resources if needed\n",
    "print(\"Checking/downloading NLTK resources (punkt, stopwords)...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"NLTK resources checked.\")\n",
    "\n",
    "\n",
    "class AspectAnalyzer:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the AspectAnalyzer with an AspectDetector.\n",
    "        Assumes AspectDetector class is already defined in the environment.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Instantiate the detector defined in a previous cell\n",
    "            self.aspect_detector = AspectDetector()\n",
    "            print(\"AspectDetector instance created successfully within AspectAnalyzer.\")\n",
    "        except NameError:\n",
    "            print(\"\\n *** ERROR: The 'AspectDetector' class is not defined. ***\")\n",
    "            print(\"Please ensure the cell defining 'AspectDetector' has been executed before this one.\")\n",
    "            raise # Re-raise the error to stop execution if AspectDetector isn't available\n",
    "        except Exception as e:\n",
    "            print(f\"\\n *** ERROR: An unexpected error occurred initializing AspectDetector: {e} ***\")\n",
    "            raise\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def extract_aspects(self, reviews_df, text_column='reviewText', category_column='category'):\n",
    "        \"\"\"\n",
    "        Extract aspects from review texts using the loaded AspectDetector.\n",
    "\n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews.\n",
    "            text_column (str): The name of the column containing review text.\n",
    "            category_column (str): The name of the column containing the category.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: The input DataFrame with an added 'detected_aspects' column (list of strings).\n",
    "                          Returns the original DataFrame if required columns are missing or an error occurs.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Starting Aspect Extraction (using '{text_column}' and '{category_column}') ---\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        if text_column not in reviews_df.columns or category_column not in reviews_df.columns:\n",
    "            print(f\"Error: Required columns '{text_column}' or '{category_column}' not found in DataFrame.\")\n",
    "            return reviews_df\n",
    "\n",
    "        # Ensure the AspectDetector was initialized correctly\n",
    "        if not hasattr(self, 'aspect_detector'):\n",
    "             print(\"Error: AspectDetector was not properly initialized. Cannot extract aspects.\")\n",
    "             return reviews_df\n",
    "\n",
    "        # Work on a copy to avoid modifying the original DataFrame directly\n",
    "        result_df = reviews_df.copy()\n",
    "        # Initialize the column with empty lists for flexibility\n",
    "        result_df['detected_aspects'] = [[] for _ in range(len(result_df))]\n",
    "\n",
    "        # Process each review\n",
    "        processed_count = 0\n",
    "        error_count = 0\n",
    "        print(f\"Processing {len(result_df)} reviews...\")\n",
    "        for index, row in result_df.iterrows():\n",
    "            text = row[text_column]\n",
    "            category = row[category_column]\n",
    "\n",
    "            # Basic check for valid input types\n",
    "            if not isinstance(text, str) or pd.isna(text) or not isinstance(category, str) or pd.isna(category):\n",
    "                # Keep the empty list in 'detected_aspects' for this row\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                # Detect aspects using the instance created in __init__\n",
    "                aspects = self.aspect_detector.detect_aspects(text, category)\n",
    "                # Ensure aspects is a list before assigning\n",
    "                if isinstance(aspects, list):\n",
    "                     result_df.loc[index, 'detected_aspects'] = aspects\n",
    "                else:\n",
    "                     # Handle cases where detect_aspects might not return a list\n",
    "                     result_df.loc[index, 'detected_aspects'] = []\n",
    "                processed_count += 1\n",
    "            except Exception as e:\n",
    "                # print(f\"Warning: Error detecting aspects for review index {index}: {e}\")\n",
    "                error_count += 1\n",
    "                # Keep the empty list on error\n",
    "\n",
    "            # Optional: Add progress indicator for long runs\n",
    "            if (index + 1) % 1000 == 0:\n",
    "                print(f\"  Processed {index + 1}/{len(result_df)} reviews...\")\n",
    "\n",
    "        end_time = time.time()\n",
    "        print(f\"--- Aspect Extraction Finished ---\")\n",
    "        print(f\"Successfully processed: {processed_count} reviews.\")\n",
    "        if error_count > 0:\n",
    "             print(f\"Encountered errors in: {error_count} reviews.\")\n",
    "        print(f\"Duration: {end_time - start_time:.2f} seconds.\")\n",
    "        return result_df\n",
    "\n",
    "    def analyze_aspects_by_category(self, reviews_df_with_aspects, category_column='category'):\n",
    "        \"\"\"\n",
    "        Analyze aspects by category. Requires 'detected_aspects' column.\n",
    "\n",
    "        Args:\n",
    "            reviews_df_with_aspects (pd.DataFrame): DataFrame containing reviews with a 'detected_aspects' column.\n",
    "            category_column (str): The name of the category column.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing aspect analysis results by category. Keys are categories.\n",
    "                  Returns empty dict if required columns are missing.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Analyzing Aspects by Category ---\")\n",
    "        if 'detected_aspects' not in reviews_df_with_aspects.columns or category_column not in reviews_df_with_aspects.columns:\n",
    "            print(f\"Error: Required columns 'detected_aspects' or '{category_column}' not found in DataFrame.\")\n",
    "            return {}\n",
    "\n",
    "        results = {}\n",
    "        valid_categories = reviews_df_with_aspects.dropna(subset=[category_column])[category_column].unique()\n",
    "        print(f\"Analyzing for categories: {list(valid_categories)}\")\n",
    "\n",
    "        # Group by category\n",
    "        for category, group in reviews_df_with_aspects.groupby(category_column):\n",
    "            # Count aspects\n",
    "            aspect_counts = Counter()\n",
    "            # Iterate safely, checking for None or non-list types\n",
    "            for aspects in group['detected_aspects']:\n",
    "                if isinstance(aspects, list):\n",
    "                    aspect_counts.update(aspects)\n",
    "\n",
    "            if not aspect_counts: # Skip category if no aspects found\n",
    "                 print(f\"  No aspects found for category: {category}\")\n",
    "                 continue\n",
    "\n",
    "            # Calculate aspect frequencies\n",
    "            total_reviews_in_group = len(group)\n",
    "            # Calculate frequency based on reviews *that had aspects detected* if desired,\n",
    "            # or total reviews in group. Using total reviews here.\n",
    "            aspect_freq = {aspect: count / total_reviews_in_group for aspect, count in aspect_counts.items()}\n",
    "\n",
    "            # Store results, sorting counts for easier viewing\n",
    "            results[category] = {\n",
    "                'aspect_counts': dict(sorted(aspect_counts.items(), key=lambda item: item[1], reverse=True)),\n",
    "                'aspect_frequencies': dict(sorted(aspect_freq.items(), key=lambda item: item[1], reverse=True)),\n",
    "                'total_reviews_in_category': total_reviews_in_group\n",
    "            }\n",
    "            print(f\"  Analyzed category: {category} ({len(aspect_counts)} unique aspects found)\")\n",
    "\n",
    "        print(\"--- Finished Aspect Analysis by Category ---\")\n",
    "        return results\n",
    "\n",
    "    def analyze_aspects_by_rating(self, reviews_df_with_aspects, rating_column='rating'):\n",
    "        \"\"\"\n",
    "        Analyze aspects by rating. Requires 'detected_aspects' column.\n",
    "\n",
    "        Args:\n",
    "            reviews_df_with_aspects (pd.DataFrame): DataFrame containing reviews with a 'detected_aspects' column.\n",
    "            rating_column (str): The name of the rating column.\n",
    "\n",
    "        Returns:\n",
    "            dict: Dictionary containing aspect analysis results by rating. Keys are ratings.\n",
    "                  Returns empty dict if required columns are missing.\n",
    "        \"\"\"\n",
    "        print(\"\\n--- Analyzing Aspects by Rating ---\")\n",
    "        if 'detected_aspects' not in reviews_df_with_aspects.columns or rating_column not in reviews_df_with_aspects.columns:\n",
    "            print(f\"Error: Required columns 'detected_aspects' or '{rating_column}' not found in DataFrame.\")\n",
    "            return {}\n",
    "\n",
    "        results = {}\n",
    "        valid_ratings = reviews_df_with_aspects.dropna(subset=[rating_column])[rating_column].unique()\n",
    "        print(f\"Analyzing for ratings: {sorted(list(valid_ratings))}\")\n",
    "\n",
    "        # Group by rating\n",
    "        for rating, group in reviews_df_with_aspects.groupby(rating_column):\n",
    "            # Count aspects\n",
    "            aspect_counts = Counter()\n",
    "            for aspects in group['detected_aspects']:\n",
    "                if isinstance(aspects, list):\n",
    "                    aspect_counts.update(aspects)\n",
    "\n",
    "            if not aspect_counts:\n",
    "                print(f\"  No aspects found for rating: {rating}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate aspect frequencies\n",
    "            total_reviews_in_group = len(group)\n",
    "            aspect_freq = {aspect: count / total_reviews_in_group for aspect, count in aspect_counts.items()}\n",
    "\n",
    "            # Store results\n",
    "            results[rating] = {\n",
    "                'aspect_counts': dict(sorted(aspect_counts.items(), key=lambda item: item[1], reverse=True)),\n",
    "                'aspect_frequencies': dict(sorted(aspect_freq.items(), key=lambda item: item[1], reverse=True)),\n",
    "                'total_reviews_with_rating': total_reviews_in_group\n",
    "            }\n",
    "            print(f\"  Analyzed rating: {rating} ({len(aspect_counts)} unique aspects found)\")\n",
    "\n",
    "        print(\"--- Finished Aspect Analysis by Rating ---\")\n",
    "        return results\n",
    "\n",
    "    def visualize_aspects(self, reviews_df_with_aspects, output_dir='./results', top_n=20):\n",
    "        \"\"\"\n",
    "        Create visualizations of aspect analysis results. Requires 'detected_aspects' column.\n",
    "\n",
    "        Args:\n",
    "            reviews_df_with_aspects (pd.DataFrame): DataFrame containing reviews with 'detected_aspects'.\n",
    "            output_dir (str): Directory to save visualizations.\n",
    "            top_n (int): Number of top aspects to show in plots.\n",
    "        \"\"\"\n",
    "        print(f\"\\n--- Visualizing Aspects (Top {top_n}) ---\")\n",
    "        if 'detected_aspects' not in reviews_df_with_aspects.columns:\n",
    "            print(\"Error: 'detected_aspects' column not found. Cannot create visualizations.\")\n",
    "            return\n",
    "        if reviews_df_with_aspects['detected_aspects'].isnull().all():\n",
    "             print(\"Warning: 'detected_aspects' column contains only null/empty values. No visualizations generated.\")\n",
    "             return\n",
    "\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        # --- Overall Aspect Distribution ---\n",
    "        print(\"Generating overall aspect distribution plot...\")\n",
    "        all_aspects_list = []\n",
    "        for aspects in reviews_df_with_aspects['detected_aspects']:\n",
    "            if isinstance(aspects, list):\n",
    "                all_aspects_list.extend(aspects)\n",
    "\n",
    "        if not all_aspects_list:\n",
    "            print(\"No aspects found in any reviews. Skipping overall distribution plot.\")\n",
    "        else:\n",
    "            aspect_counts = Counter(all_aspects_list)\n",
    "            aspect_df = pd.DataFrame(aspect_counts.items(), columns=['aspect', 'count'])\\\n",
    "                          .sort_values('count', ascending=False)\n",
    "\n",
    "            plt.figure(figsize=(12, max(6, len(aspect_df.head(top_n)) * 0.4))) # Adjust height\n",
    "            sns.barplot(x='count', y='aspect', data=aspect_df.head(top_n), palette='viridis')\n",
    "            plt.title(f'Top {min(top_n, len(aspect_df))} Aspects Mentioned Overall')\n",
    "            plt.xlabel('Total Count')\n",
    "            plt.ylabel('Aspect')\n",
    "            plt.tight_layout()\n",
    "            try:\n",
    "                plt.savefig(os.path.join(output_dir, 'aspect_distribution_overall.png'))\n",
    "                print(f\"Saved: aspect_distribution_overall.png\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving aspect_distribution_overall.png: {e}\")\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        # --- Aspect by Category ---\n",
    "        if 'category' in reviews_df_with_aspects.columns:\n",
    "            print(\"\\nGenerating aspect distribution by category plot...\")\n",
    "            category_results = self.analyze_aspects_by_category(reviews_df_with_aspects)\n",
    "\n",
    "            if not category_results:\n",
    "                print(\"No category analysis results. Skipping aspect by category plot.\")\n",
    "            else:\n",
    "                category_aspect_data = []\n",
    "                for category, results_dict in category_results.items():\n",
    "                    # Take top N aspects for this specific category\n",
    "                    top_aspects_cat = list(results_dict['aspect_counts'].items())[:top_n]\n",
    "                    for aspect, count in top_aspects_cat:\n",
    "                        category_aspect_data.append({\n",
    "                            'category': category,\n",
    "                            'aspect': aspect,\n",
    "                            'count': count\n",
    "                        })\n",
    "\n",
    "                if not category_aspect_data:\n",
    "                     print(\"No aspect data found across categories. Skipping category plot.\")\n",
    "                else:\n",
    "                    category_aspect_df = pd.DataFrame(category_aspect_data)\n",
    "\n",
    "                    # Determine plot dimensions based on number of categories and aspects\n",
    "                    num_categories = category_aspect_df['category'].nunique()\n",
    "                    plot_height = max(6, num_categories * 1.5) # Adjust height based on categories\n",
    "                    plot_width = max(10, top_n * 0.6) # Adjust width based on aspects shown\n",
    "\n",
    "                    plt.figure(figsize=(plot_width, plot_height))\n",
    "                    # Use barplot directly for better control over figure size\n",
    "                    sns.barplot(x='count', y='category', hue='aspect', data=category_aspect_df,\n",
    "                                dodge=False) # Use dodge=False if too many aspects overlap, or adjust width\n",
    "                    plt.title(f'Top {top_n} Aspects by Category')\n",
    "                    plt.xlabel('Count within Category')\n",
    "                    plt.ylabel('Category')\n",
    "                    plt.legend(title='Aspect', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for legend\n",
    "                    try:\n",
    "                        plt.savefig(os.path.join(output_dir, 'aspect_by_category.png'))\n",
    "                        print(f\"Saved: aspect_by_category.png\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving aspect_by_category.png: {e}\")\n",
    "                    plt.close()\n",
    "\n",
    "\n",
    "        # --- Aspect by Rating ---\n",
    "        if 'rating' in reviews_df_with_aspects.columns:\n",
    "            print(\"\\nGenerating aspect distribution by rating plot...\")\n",
    "            rating_results = self.analyze_aspects_by_rating(reviews_df_with_aspects)\n",
    "\n",
    "            if not rating_results:\n",
    "                print(\"No rating analysis results. Skipping aspect by rating plot.\")\n",
    "            else:\n",
    "                rating_aspect_data = []\n",
    "                for rating, results_dict in rating_results.items():\n",
    "                    # Take top N aspects for this specific rating\n",
    "                    top_aspects_rat = list(results_dict['aspect_counts'].items())[:top_n]\n",
    "                    for aspect, count in top_aspects_rat:\n",
    "                        rating_aspect_data.append({\n",
    "                            'rating': str(rating), # Convert rating to string for categorical plotting\n",
    "                            'aspect': aspect,\n",
    "                            'count': count\n",
    "                        })\n",
    "\n",
    "                if not rating_aspect_data:\n",
    "                    print(\"No aspect data found across ratings. Skipping rating plot.\")\n",
    "                else:\n",
    "                    rating_aspect_df = pd.DataFrame(rating_aspect_data)\n",
    "\n",
    "                    plt.figure(figsize=(12, 7)) # Adjust size as needed\n",
    "                    sns.barplot(x='rating', y='count', hue='aspect', data=rating_aspect_df, dodge=True)\n",
    "                    plt.title(f'Top {top_n} Aspects by Rating')\n",
    "                    plt.xlabel('Rating')\n",
    "                    plt.ylabel('Count within Rating')\n",
    "                    plt.legend(title='Aspect', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "                    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout for legend\n",
    "                    try:\n",
    "                        plt.savefig(os.path.join(output_dir, 'aspect_by_rating.png'))\n",
    "                        print(f\"Saved: aspect_by_rating.png\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error saving aspect_by_rating.png: {e}\")\n",
    "                    plt.close()\n",
    "\n",
    "        print(\"--- Finished Aspect Visualizations ---\")\n",
    "\n",
    "\n",
    "# ==============================================================\n",
    "# Example usage in the `if __name__ == \"__main__\":` block\n",
    "# ==============================================================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n--- Running Aspect Analysis Example ---\")\n",
    "\n",
    "    # Define input file and output directory\n",
    "    input_file = \"filtered_amazon_reviews.parquet\"\n",
    "    output_directory = \"./results\"\n",
    "\n",
    "    # --- Try loading data ---\n",
    "    reviews_df = None\n",
    "    try:\n",
    "        print(f\"Attempting to load reviews from: {input_file}\")\n",
    "        reviews_df = pd.read_parquet(input_file)\n",
    "        # Optional: Sample the data for faster testing\n",
    "        # sample_size = 1000\n",
    "        # if len(reviews_df) > sample_size:\n",
    "        #     print(f\"Sampling {sample_size} reviews for testing...\")\n",
    "        #     reviews_df = reviews_df.sample(n=sample_size, random_state=42)\n",
    "        print(f\"Successfully loaded {len(reviews_df)} reviews.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Input reviews file not found at '{input_file}'.\")\n",
    "        print(\"Please ensure the file exists or run the data preparation step first.\")\n",
    "    except Exception as e:\n",
    "         print(f\"Error loading Parquet file '{input_file}': {e}\")\n",
    "\n",
    "\n",
    "    # --- Proceed only if data is loaded and AspectDetector is likely available ---\n",
    "    if reviews_df is not None and not reviews_df.empty:\n",
    "        analyzer = None\n",
    "        try:\n",
    "            # --- Initialize Analyzer (this requires AspectDetector) ---\n",
    "            analyzer = AspectAnalyzer()\n",
    "\n",
    "            # --- Extract Aspects ---\n",
    "            # Use the analyzer instance to call extract_aspects\n",
    "            result_df_with_aspects = analyzer.extract_aspects(reviews_df) # Modify the dataframe in place (via copy)\n",
    "\n",
    "            # --- Visualize Aspects ---\n",
    "            # Check if aspect extraction was successful before visualizing\n",
    "            if 'detected_aspects' in result_df_with_aspects.columns:\n",
    "                 analyzer.visualize_aspects(result_df_with_aspects, output_dir=output_directory)\n",
    "\n",
    "                 # --- Print Sample Results ---\n",
    "                 print(\"\\n--- Sample Results with Detected Aspects ---\")\n",
    "                 # Ensure 'detected_aspects' column exists before sampling/printing\n",
    "                 sample = result_df_with_aspects.sample(min(5, len(result_df_with_aspects))) # Sample up to 5 rows\n",
    "                 required_cols = ['reviewText', 'category', 'detected_aspects']\n",
    "                 if all(col in sample.columns for col in required_cols):\n",
    "                     for index, row in sample.iterrows():\n",
    "                         text_preview = row['reviewText'][:100] if isinstance(row['reviewText'], str) else \"[No Text]\"\n",
    "                         aspects_list = row['detected_aspects'] if isinstance(row['detected_aspects'], list) else []\n",
    "                         print(f\"\\nReview Index {index}: {text_preview}...\")\n",
    "                         print(f\"  Category: {row['category']}\")\n",
    "                         print(f\"  Detected aspects: {aspects_list}\")\n",
    "                 else:\n",
    "                      print(\"Could not display sample results as required columns are missing after processing.\")\n",
    "\n",
    "            else:\n",
    "                print(\"\\nAspect extraction did not produce the 'detected_aspects' column. Skipping visualization and sample results.\")\n",
    "\n",
    "        except NameError:\n",
    "            # This catches the error if AspectDetector wasn't defined before AspectAnalyzer was initialized\n",
    "            print(\"\\nExecution stopped: AspectAnalyzer could not be initialized because AspectDetector was not found.\")\n",
    "            print(\"Please ensure the cell defining AspectDetector is run first.\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\nAn unexpected error occurred during analysis: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "\n",
    "    else:\n",
    "        print(\"\\nAnalysis cannot proceed as review data was not loaded.\")\n",
    "\n",
    "    print(\"\\n--- Aspect Analysis Example Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9061bb36-d583-4a74-931c-af83e88fc992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "from datetime import datetime\n",
    "from data_loader import load_amazon_reviews_hf, preprocess_reviews\n",
    "from aspect_analysis import AspectAnalyzer\n",
    "from emotion_analyzer import EmotionAnalyzer\n",
    "from network_analysis import ReviewNetworkAnalyzer\n",
    "from domain_adapter import FakeReviewDetector\n",
    "from context_aware_sentiment import SelfAdaptiveModel\n",
    "\n",
    "class BatchProcessor:\n",
    "    def __init__(self, output_dir='./results'):\n",
    "        \"\"\"\n",
    "        Initialize the BatchProcessor.\n",
    "        \n",
    "        Args:\n",
    "            output_dir (str): Directory to save results.\n",
    "        \"\"\"\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize analyzers\n",
    "        self.aspect_analyzer = AspectAnalyzer()\n",
    "        self.emotion_analyzer = EmotionAnalyzer()\n",
    "        self.network_analyzer = ReviewNetworkAnalyzer()\n",
    "        self.fake_detector = FakeReviewDetector()\n",
    "        self.sentiment_model = SelfAdaptiveModel()\n",
    "        \n",
    "        print(f\"BatchProcessor initialized. Results will be saved to {output_dir}\")\n",
    "    \n",
    "    def process_reviews(self, reviews_df, sample_size=None):\n",
    "        \"\"\"\n",
    "        Process reviews in batch mode.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews.\n",
    "            sample_size (int): Number of reviews to sample (optional).\n",
    "            \n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame with analysis results.\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        print(f\"Starting batch processing of {len(reviews_df)} reviews...\")\n",
    "        \n",
    "        # Sample if needed\n",
    "        if sample_size and len(reviews_df) > sample_size:\n",
    "            reviews_df = reviews_df.sample(sample_size, random_state=42)\n",
    "            print(f\"Sampled {sample_size} reviews for processing.\")\n",
    "        \n",
    "        # 1. Aspect Analysis\n",
    "        print(\"Performing aspect analysis...\")\n",
    "        reviews_df = self.aspect_analyzer.extract_aspects(reviews_df)\n",
    "        self.aspect_analyzer.visualize_aspects(reviews_df, self.output_dir)\n",
    "        \n",
    "        # 2. Emotion Analysis\n",
    "        print(\"Performing emotion analysis...\")\n",
    "        reviews_df = self.emotion_analyzer.analyze_reviews(reviews_df)\n",
    "        self.emotion_analyzer.visualize_emotions(reviews_df, self.output_dir)\n",
    "        \n",
    "        # 3. Network Analysis\n",
    "        print(\"Performing network analysis...\")\n",
    "        network_results = self.network_analyzer.analyze_review_network(reviews_df, self.output_dir)\n",
    "        \n",
    "        # 4. Fake Review Detection\n",
    "        print(\"Performing fake review detection...\")\n",
    "        if 'text' in reviews_df.columns and 'reviewer_id' in reviews_df.columns and 'timestamp' in reviews_df.columns:\n",
    "            is_fake = self.fake_detector.analyze_fake_review_patterns(reviews_df)\n",
    "            reviews_df['is_fake'] = is_fake\n",
    "            \n",
    "            # Calculate suspicious score\n",
    "            suspicious_scores = np.zeros(len(reviews_df))\n",
    "            if 'suspicious_clusters' in network_results:\n",
    "                for cluster in network_results['suspicious_clusters']:\n",
    "                    for reviewer_id in cluster:\n",
    "                        suspicious_scores[reviews_df['reviewer_id'] == reviewer_id] += 1\n",
    "            \n",
    "            reviews_df['suspicious_score'] = suspicious_scores\n",
    "            \n",
    "            # Visualize fake review distribution\n",
    "            self._visualize_fake_reviews(reviews_df)\n",
    "        \n",
    "        # 5. Context-Aware Sentiment Analysis\n",
    "        print(\"Performing context-aware sentiment analysis...\")\n",
    "        reviews_df['sentiment_score'] = reviews_df.apply(\n",
    "            lambda row: self.sentiment_model.analyze_with_context(\n",
    "                row['reviewText'] if 'reviewText' in row else row['text'],\n",
    "                row['category'] if 'category' in row else None\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Map sentiment scores to labels\n",
    "        reviews_df['sentiment'] = reviews_df['sentiment_score'].apply(\n",
    "            lambda x: 'positive' if x > 0.6 else ('negative' if x < 0.4 else 'neutral')\n",
    "        )\n",
    "        \n",
    "        # Save processed data\n",
    "        output_file = os.path.join(self.output_dir, 'processed_reviews.parquet')\n",
    "        reviews_df.to_parquet(output_file)\n",
    "        print(f\"Processed data saved to {output_file}\")\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"Batch processing completed in {elapsed_time:.2f} seconds.\")\n",
    "        \n",
    "        return reviews_df\n",
    "    \n",
    "    def _visualize_fake_reviews(self, reviews_df):\n",
    "        \"\"\"\n",
    "        Create visualizations for fake review analysis.\n",
    "        \n",
    "        Args:\n",
    "            reviews_df (pd.DataFrame): DataFrame containing reviews with fake detection results.\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        if 'is_fake' not in reviews_df.columns:\n",
    "            return\n",
    "        \n",
    "        # Rating distribution: fake vs genuine\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(\n",
    "            data=reviews_df, \n",
    "            x='rating', \n",
    "            hue='is_fake',\n",
    "            multiple='dodge',\n",
    "            shrink=0.8,\n",
    "            discrete=True\n",
    "        )\n",
    "        plt.title('Rating Distribution: Fake vs. Genuine Reviews')\n",
    "        plt.xlabel('Rating')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(['Genuine', 'Fake'])\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(self.output_dir, 'rating_distribution.png'))\n",
    "        plt.close()\n",
    "        \n",
    "        # Fake review percentage by category\n",
    "        if 'category' in reviews_df.columns:\n",
    "            plt.figure(figsize=(12, 6))\n",
    "            fake_by_category = reviews_df.groupby('category')['is_fake'].mean() * 100\n",
    "            fake_by_category = fake_by_category.sort_values(ascending=False)\n",
    "            \n",
    "            sns.barplot(x=fake_by_category.index, y=fake_by_category.values)\n",
    "            plt.title('Fake Review Percentage by Category')\n",
    "            plt.xlabel('Category')\n",
    "            plt.ylabel('Fake Review Percentage')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(os.path.join(self.output_dir, 'category_distribution.png'))\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Batch process Amazon reviews for analysis.')\n",
    "    parser.add_argument('--categories', type=str, nargs='+', \n",
    "                        default=['All Beauty', 'Health and Personal Care', 'Appliances', 'Video_Games'],\n",
    "                        help='Categories to analyze')\n",
    "    parser.add_argument('--sample_size', type=int, default=1000, \n",
    "                        help='Number of reviews to sample for analysis')\n",
    "    parser.add_argument('--output_dir', type=str, default='./results',\n",
    "                        help='Directory to save results')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Load reviews\n",
    "    print(f\"Loading reviews for categories: {args.categories}\")\n",
    "    reviews_df = load_amazon_reviews_hf(args.categories)\n",
    "    \n",
    "    # Preprocess reviews\n",
    "    reviews_df = preprocess_reviews(reviews_df)\n",
    "    \n",
    "    # Process reviews\n",
    "    processor = BatchProcessor(output_dir=args.output_dir)\n",
    "    processed_df = processor.process_reviews(reviews_df, sample_size=args.sample_size)\n",
    "    \n",
    "    print(f\"Batch processing complete. Processed {len(processed_df)} reviews.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83ba66c-8638-47ac-93f6-7ed564d240e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef518e8a-e8fc-404b-9c9c-c192f812fa65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
